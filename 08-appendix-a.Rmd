# Developing the survey tool

```{r 08-setup, include=FALSE}
library(plyr)
library(tidyverse)
library(lubridate)
library(readxl)
library(knitr)
library(kableExtra)
library(ggthemes)
library(plotly)
library(mousetrap)
library(randomizr)
library(data.table)
library(lavaan)
library(survival)
library(survminer)
library(papaja)
library(tidytidbits)


knitr::opts_chunk$set(echo = FALSE, cache = FALSE, warning = FALSE, message = FALSE)
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'), 'packages.bib')
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')

########################################
################# DATA #################
########################################
feature_retention <- 
  read_xlsx("../3 eshot/PRA_Results/FeatureRetention.xlsx", 
            sheet = "Sheet1")[1:150,]

Group1to4 <- readRDS("chapter_3_data/Group1to4.rds")

group_5 <- readRDS("chapter_3_data/Group5.rds")
group_5_gta = read_csv("../3 eshot/Data/Group5_GTA_v3.csv") %>% 
    filter(!CandidateId %in% 
           c("188254", "292087","130036","184996", "258131", "193950", "112773",
             "188235") #inappropriate response to personal projects
         & is.na(RegExpTimeToAssess) == FALSE)
group_5_pra <- group_5 %>% 
  filter(CandidateId %in% group_5_gta$CandidateId &
           is.na(t12_mtn_percent_1) == FALSE)


Group5_FTP = read_csv("../3 eshot/Data/Group5_FTP_v3.csv")
G5_female_GTA_18m_comb <- read_csv("../3 eshot/Data/G5_female_GTA_18m_comb_v3.csv")
G5_female_GTA_18m_train <- read_csv("../3 eshot/Data/G5_female_GTA_18m_train_v3.csv")
G5_male_GTA_18m_train <- read_csv("../3 eshot/Data/G5_male_GTA_18m_train_v3.csv")
G5_male_GTA_18m_test <- read_csv("../3 eshot/Data/G5_male_GTA_18m_test_v3.csv")
G5_male_GTA_18m_pred <- read_csv("../3 eshot/Data/G5_male_GTA_18m_pred_v3.csv")
g5_zsex_ftp_consol <- read_csv("../3 eshot/Data/G5_FTP_consol_v3.csv")


### PRA results ###
#### Feature subsets ####
g5_female_gta_18m_fs <- 
  read_csv("../3 eshot/PRA_Results/v3/gta_female_g5_18m_FS.csv", 
           col_types = "cci")
g5_male_gta_18m_fs <- 
  read_csv("../3 eshot/PRA_Results/v3/gta_male_g5_18m_FS.csv", 
           col_types = "cci")

g5_zsex_ftp_fs <- 
  read_csv("../3 eshot/PRA_Results/v3/ftp_g5_18m_FS.csv", 
           col_types = "cci")

#### Classification rates ####
G5_female_GTA18m_PRA <- 
  read_xlsx("../3 eshot/PRA_Results/v3/g5_female_gta18m_comb_classification.xlsx", 
            sheet = "Sheet1", range = "A10:J38")  %>% 
  left_join(g5_female_gta_18m_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median)


G5_male_GTA18m_train_PRA <- 
  read_xlsx("../3 eshot/PRA_Results/v3/g5_male_gta18m_train_classification.xlsx", 
            sheet = "Sheet1", range = "A10:L38") %>% 
  left_join(g5_male_gta_18m_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median)

G5_zsex_ftp_train_PRA <- 
  read_xlsx("../3 eshot/PRA_Results/v3/g5_zsex_consol_ftp_classification.xlsx", 
            sheet = "Sheet1", range = "A10:J36") %>% 
  left_join(g5_zsex_ftp_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median)

G5_male_GTA18m_valid_PRA <- 
  read_excel("../3 eshot/PRA_Results/v3/g5_male_gta18m_validation_classification.xlsx")

#### PRA predictions ####
g5_male_GTA_18m_pred_perform <- 
  read_rds("../3 eshot/PRA_Results/v3/g5_male_gta_18m_pred/g5_male_GTA_18m_pred_perform.rds")


#### Other ####
GTA_Survival <- 
  read_rds("chapter_3_data/GTA_Survival.rds")

eshot_vars <- 
  read_xlsx("../3 eshot/Design/eshot_variables.xlsx", 
            sheet = "Sheet1", na = "NA") %>%
  mutate(full_model = full_model %>% str_remove_all("\r"),
    short_model = short_model %>% str_remove_all("\r"))

#############################################
################# FUNCTIONS #################
#############################################
normalize <- 
  function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
  }

##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- 
  function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
           conf.interval=.95, .drop=TRUE) {
    

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- 
      function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
      }
    
    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- plyr::ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- plyr::rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
  }

if (is_latex_output()) {font = "serif" } else {font = "sans"}

```

## Study 1 - Item selection

The aim of this study was to identify a suitable measure for each construct identified in Chapter 2 **LINK**, which could then be used to identify the most important variables for discriminating candidates who do complete the ML qualification from those who do not. Given that we wanted to collect data from candidates for `r nrow(eshot_vars)` variables, using full-length measures of the relevant constructs would be unreasonable for participants. To measure each construct of interest with a full length measure would create a survey so long that few candidates would complete it and those that did would likely not be representative of the population. Horvath and Röthlin [-@Horvath2018] discuss additional issues associated with using "basic research questionnaires" that are not best suited to use in an applied sport psychology setting, where instead, one may only want to use a questionnaire as a screening tool, or help provide "a complete picture about an athlete's situation." As such we were particularly interested in identifying short-form measures as using such measures was most likely to allow us to create a suitably short survey to collect data with. 

The development of short-form measures to reduce the burden on participants has been of interest to researchers for over 100 years [@Smith2000]. However, the development of short-form measures has attracted some criticism [e.g., @Levy1968; @Smith2000; @Wechsler1967]. One of the main criticisms of short-form measures has been that "rigorous, valid, comprehensive assessment is crucial for the evaluation and treatment of many psychological problems" [@Smith2000, p 102] and that the time saving afforded by a short-form measure does not warrant the loss of validity associated with measuring a construct with fewer items.

When creating, or identifying, a short-form measure one should not assume that the evidence for the validity and reliability of the original measure applies to the short-from, therefore it is important to provide evidence for the reliability and validity of the short-form [@Smith2000]. This evidence should include, but is not limited to, reliability of the short-from, shared variance between the full- and short-form measure, content validity/coverage of the construct, and also that the reduction in items offers a meaningful reduction in the time taken for the measure to be completed [@Horvath2018; @Smith2000].

This project's aim was to identify the most important discriminatory variables for identifying candidates who do or do not complete the ML qualification using Machine Learning techniques rather than testing the relationships between variables using regression based techniques or structural equation modelling. Therefore, instead of using full-length measures to collect data for each construct, we used one or two item *indicators* for each construct, unless a suitable short-form measure existed [e.g., the Ten Item Personality Inventory @Gosling2003]. 


```{r study-1-data-sets}
### FUNCTIONS ###
prepare_lavaan <- 
  function(data_path, pattern = NULL, case = "snake"){
    # function to read and prepare data for CFA
    require(tidyverse)
    require(janitor)
    
    if (is.null(pattern)) {
      read_csv(data_path) %>% 
        na.omit() %>% 
        clean_names(case = case) %>% 
        scale() %>% 
        as.data.frame
    } 
    else 
      if (!is.null(pattern)) {
        read_csv(data_path) %>% 
          na.omit() %>% 
          clean_names(case = case) %>% 
          select(starts_with(pattern)) %>%
          scale() %>% 
          as.data.frame()
    }
  }

### DATA ###
PASSQ <- 
  prepare_lavaan(
    data_path = "../3 eshot/Design/Validation data/Social Support/PASSQ_ARSQ.csv",
                 pattern = "pass")

ARSQ <- 
  prepare_lavaan(
               data_path = "../3 eshot/Design/Validation data/Social Support/PASSQ_ARSQ.csv",
               pattern = "received")

MCBS <-  prepare_lavaan(
               data_path = "../3 eshot/Design/Validation data/MCBS/Coach_Study2_1.csv")

colnames(MCBS) <- 
  c("Clust", "ob1", "ob2", "ob3", "ob4", "eq1", "eq2", "eq3", "eq4","gs1",
    "gs2", "gs3", "gs4", "gs5", "df1", "df2", "df3", "df4", "mf1", "mf2",
    "mf3", "mf4", "Obeservation", "Effective Questioning", "Goal Setting",
    "Developmental Feedback", "Motivational Feedback")

TROSCI <- 
  read_csv("../3 eshot/Design/Validation data/TROSCI/Maulin and Sholto.csv") %>% 
  psych::reverse.code(items = ., keys = c(-1,-1,-1,1,-1,1,1,1,1,1,1,-1,1,-1,1), 
                      mini = 1, maxi = 9) %>% 
  data.frame() %>% 
  scale()

colnames(TROSCI) <- 
  c("trosci_1", "trosci_2", "trosci_3", "trosci_4", "trosci_5", "trosci_6",
    "trosci_7", "trosci_8", "trosci_9", "trosci_10", "trosci_11", "trosci_12", 
    "trosci_13", "trosci_14", "trosci_15")

FMPS <- 
  read_csv("../3 eshot/Design/Validation data/FMPS/FMPS.csv", 
           col_names = paste0("fmps_", 1:35)) %>% 
  scale()

BRS <- 
  read_csv("../../3 Robustness/Study 3/Data/SPSP.csv") %>% 
  select(starts_with("BRS")) %>% 
  scale()

colnames(BRS) <- c(paste0("brs_", 1:6), "BRS")

conflict <- read_csv("../3 eshot/Design/Validation data/conflict/matt_conflict_data.csv", na = "999") %>% 
  na.omit()

conflict[,3:16] <- sapply(conflict[,3:16], scale)


study1_data_sets <- list(PASSQ, ARSQ, MCBS, TROSCI, FMPS, BRS, conflict)
```


### Method

#### Procedure

Although using full measures was not a realistic aim in the project, we still felt that the reliability and validity of the indicators that we would use was paramount. Researchers have suggested a variety of ways in which short-form measures can be developed whilst remaining both reliable and valid. Considering the guidance provided by @Smith2000 and @Horvath2018 along with the aim of this research, we used the steps below to identify items which would be used to collect data from candidates.

Our first choice was to identify existing suitable short-form measure. When this was not possible, but there was an existing measure that we were able to access secondary data for, we used the following steps. Firstly, we checked that existing measure did measure the construct of interest and that there was sufficient evidence for its reliability and validity. Secondly, we identified which items we wanted to retain based on both content validity and factor loadings. It was important that the items retained still provided adequate coverage of the construct. In some instances, this meant retaining an item which had a (relatively) low factor loading, but measured a unique aspect of that construct, was more important than it was to retain items with high factor loadings. This will necessarily have lowered the reliability coefficient for the short-form measure, however, it is important to note that internal consistency is only one aspect of valididty. If we did not feel that this process left us with items that provided adequate coverage of the construct of interest, we made a note of how the new, narrower, construct differed from the original.

Once we had identifed the items we wished to retain, we fitted a single factor latent variable model for both the full- and short-form measure to the secondary data, using `lavaan` [@R-lavaan], to estimate factor scores for each participant. These factor scores were then used to calculate a Pearson's correlation coefficient  between predicted factor sores for full- and short-form measure as an estimate of shared variance. We believe that this method is better than correlating the item sum-scores as latent variables account for measurement error, thus, reducing the likelihood of receiving an optimistically biased estimate due to error correlation. Shared variance with the full measure was our main concern for this study as if the correlations are high enough then the two measures can be thought of as approximately equal [@Smith2000]. Finally, we calculated the composite reliability for the new short-form measure [$\omega$; @Fornell1981]^[Horvath and Röthlin [-@Horvath2018] highlight the fact that a measure of internal consistency such as Cronbach's alpha may be lower for a short-form measure simply because it has fewer items, therefore this may be a less useful metric than it would be in a longer measure. Composite reliability does not include the number of items in its calculation therefore is more suitable for assessing the internal consistency of short-form measures.].

If secondary data were not available but we identified a suitable measure, we chose the best item(s) based on face validity of the items and factor loadings reported in the original paper validating the full measure. Finally, if none of the options above were possible, we developed item(s) within the research team in collaboration with Mountain Training UKI.


#### Measures

> (How much detail do I need to go into in the section - particularly for scales that already exist? Is it enough to say that there is a list of variables and measures in the supplementary information?)
> (Talk about domains, full information available in supplementary information.)

##### Personality measures

* To measure the "Big-Five" personality traits (openness, conscientiousness, extraversion, agreeableness and emotional stability), we used the Ten Item Personality Inventory [TIPI; @Gosling2003]. The TIPI comprises ten pairs of items (e.g., "Critical, quarrelsome"), one positively worded and one negatively worded for each trait. Each item has the same stem, "I see myself as..." Participants are then asked to score each item on a seven-point Likert scale from "Disagree strongly" (1) to "Agree strongly" (7) and sum scores are calculated for each of the five traits.
* Resilience and Robustness - Brief Resilience Scale [BRS; @Smith2008]
* Perfectionism (Personal Standards and Concern Over Mistakes) - Frost Multidimensional Perfectionism Scale [FMPS; @Frost1990]
  * **Do I want to include something about reanlaysing the YIPS study and finding similar results? What is the difference in classification rates/discriminant function?**
* Robustness of confidence - Trait Robustness of Self-Confidence Inventory [TROSCI; @Beattie2011]


##### Socio-demographic

Some socio-demographic data are available on the CMS; however, some is not (e.g., income level, education level). To measure these, we used standard socio-demographic questions (e.g., "What is the highest level of school you had completed or the highest degree you had received when you registered?")


##### Self-efficacy scale

[//]: # (Ross: I'd turn this paragraph a little on its head. Start with following Banduras guidelines and the importance of creating a situation specific measure. Then say how you achieved creating this measure)

Perceived self-efficacy is domain specific and individuals will have varying levels of self-effiacy beliefs across different domains of their lives, therefore it is important that any measure of perceived self-efficacy is domain specific [@Bandura1997; @Bandura2006]. Mountain Training provide clear documentation about what will be required of candidates during their assessment, this includes a candidate handbook and syllabus [@MountainTrainingUK2015a], and a separate skills checklist [@MountainTrainingUK2015]. The first author conducted an inductive content analysis [@Cho2014] of these documents to identify a list of skills, which should be able to perform on an ML assessment. This list of skills was then discussed with Mountain Training UKI who agreed that it provided good coverage of the skills that would be covered on an assessment. 

Using the list of skills, a self-efficacy scale was created following Bandura's [-@Bandura2006] guidelines. The resultant scale was then piloted with Mountain Training UKI staff who provided feedback on the items, which was then collated and used to refine the scale. The final scale was made up of eleven items (e.g., "lead a group effectively in the mountains") rated on a scale of 0 (could not do at all) to 100 (highly certain could do) with a mid-point anchor (50; moderately could do). The items could then be presented to participants three times, each with a different introduction as we wanted to measure efficacy at two points along the pathway and also candidates' ideal efficacy levels:

1) Please rate how confident you were that you could do them immediately after your training course.
2) Please rate your degree of confidence, as of now/at your (first) assessment^[Different wording was presented to candidates based on whether or not they had been assessed.].
3) Now we know about your levels of confidence to perform these tasks as of now/at your (first) assessment, we would like to understand how confident you feel that your ideal self would be/have been at your (first) assessment. The Ideal Self: “Your ideal self is the kind of person you’d really like to be. It is defined by the characteristics you would ideally like to have. It’s not necessary that you have these characteristics now, only that you believe you want to have them."


##### Personal projects

We used a modified version of Little's [-@Little1983] Personal Project Analysis, similar to that used by Beattie et al. [-@Beattie2015]. The instructions were adapted and read:

> We are interested in studying the kinds of personal projects that candidates have at different stages of their life and how they relate to candidates’ motivation to become an ML. All of us have a number of personal projects at any given time that we think about, plan for, and sometimes (though not always) complete. 
>
> Please take a moment to think about the projects or goals that you were working on before your assessment, these may include things that you have already told us about.

Participants were then given examples of goals (e.g., "Completing another outdoor qualification," "Spending more time with my family") and asked to "write down the two goals that you were most likely to work towards in the six-months before your assessment, *not-including* becoming an ML." On the following page, for each of their stated goals and for the goal of "becoming an ML," they were then asked to rate the: importance of the goal, "not at all important to me" (0) to "extremely important to me" (100); progress towards the goal in the last six months/six months before their assessment, "no progress" (0) to "most progress" (100); and their perceived self-efficacy of attaining the goal, "I definitely do/did not have" (0) to "I definitely have/had" (100). 

Using the scores provided, the following can then be calculated: relative importance, relative progress, and relative efficacy score using the following formula^[To avoid returning an undefined value one is added to both the numerator and denominator.]:

$$
Relative = \frac{Mountain\,Leader + 1 }{(Goal\,1 + Goal\,2) \div 2 + 1}
$$


##### Motives

In Chapter 2 it appeared that two different levels of motive were important to the completion of the ML qualification: participatory (the goal content) and regulatory (the "why"). To measure the participatory motives, we employed a similar methodology to Sheldon and Elliot's [-@Sheldon1999] adaptation of Little's [-@Little1983] Personal Project Analysis. First, we asked participants to list two goals that they hoped to achieve by registering for the ML qualification. These reasons were then coded by the first author on a scale of definitely intrinsic (1) to definetly extrinsic (5) and a mean score was calculated. Examples for each value are as follows: (1) "To have fun," (2) "Being better equipped to enjoy the mountains safely for myself,"(3) "Assessing my own ability,"(4) "Confidence in leading groups in the moutnains,"(5) "Gain the ML qualification."

On the next page of the survey participants were asked to rate each reason they had given in terms of their behavioural regulation. Each item had the same stem, "I pursue this goal because..." The intrinsic item was "of the fun and enjoyment it provides me," the integrated reason was "it is a part of who I am or aspire to be," the identified reason was "I really believe it's an important goal to have," the interjected reason was "I would feel ashamed, guilty, or anxious if I didn't," the external reason was "someone else wants me to or because the situation demands it." Participants scored each of these reasons on a visual analogue scale with five equally spaced anchors from strongly agree (0) to strongly disagree (100), a mean score for each of the regulatory motives was then calculated.


##### Course staff coaching behaviours

The Military Coaching Behaviour Scale [MCBS; @Wagstaff2018] is a 22-item scale that assesses five coaching behaviours: observing and performance analysis, effective questioning, goal setting, developmental feedback, and motivational feedback.


##### Need supportive environment

The Perceived Environmental Supportiveness Scale [PESS; @Markland2010] measures autonomy support, structure, and involvement, each with five items.


##### Perceived conflict on courses

* Stuff from Matt
* Intra group conflict scale for sport (manuscript in prep Boulder, Hardy, Roberts, Woodman)


##### Social Support

* The Perceived Available Support in Sport Questionnaire [PASSQ; @Freeman2011]
* The Athletes' Received Support Questionnaire [ARSQ; @Freeman2014]


##### Preparation for assessment

Preparation for an assessment may encompass a variety of different things for different candidates and we were interested in how much candidates felt that they had done to prepare for an assessment. We asked participants to complete the sentence, "I have done ____ to prepare effectively for an ML assessment" using a visual analogue scale, anchored at "nothing" and "all that I could." Given the complex nature of this question, we first asked participants to list some of the things that they had done in the last six-months/six-months prior to their assessment to prepare. This was done to help improve accuracy in responses in a similar fashion to the decomposition questions used in The World Health Organization Health and Work Performance Questionnaire [@Kessler2003a; @Means1991].


##### Life events

Based on the results of Chapter 2, we wanted to measure change in three domains of candidates lives: social, professional, and health. The Recent Life Change Questionnaire [RLCQ; @Miller1997] has items covering these domains. At this point, we were not concerned about the exact events that may, or may not, have occurred, therefore we presented items from the RLCQ as examples for each domain and then asked participants to rate the extent to which they had experienced change in that domain of their life since their training course using a visual analogue scale (no change to major change). Another consideration when choosing this method was the sensitive nature of some life events. Allowing participants to indicate a magnitude of perceived change rather than explicitly responding to a sensitive item (e.g., "Miscarriage or abortion," "Being held in jail") was deemed more appropriate for this study.


##### Aspirations, intentions, and expectations


#### Data


### Results

```{r short-measure-correlations}

### FUNCTIONS ###
latent_cor <-
  function(data = eval(parse(text = measure_code)), full_model = full_model, short_model = short_model) {
    
    require(lavaan)
    
    full_fit <-
      cfa(full_model, data = data, std.lv = FALSE)
    
    if (str_detect(short_model, "=~") == TRUE) {
      short_fit <-
        cfa(short_model, data = data, std.lv = FALSE)
      
      cor_tab <-
        cor.test(predict(full_fit), predict(short_fit))
    }
    else stop("no short latent variable found")
    
    return(cor_tab)
  }


latent_cor_comp_rel <- 
    function(data = eval(parse(text = measure_code)), model) {
    
      if (str_detect(model, "\\+") == TRUE) {
        require(lavaan)
        
        fit <-
          cfa(model = model, data = data, std.lv = FALSE)
        
        comp_rel <-
          sum(fit@Model@GLIST$lambda)^2 /
          (sum(fit@Model@GLIST$lambda)^2 +
             sum(1-(fit@Model@GLIST$lambda^2)))
      } else
        comp_rel <- NA
    return(comp_rel)
  }

### ANALYSIS
eshot_vars %>% 
  #filter(short_available == FALSE) %>% # variables that need a short-form measure creating
  filter(!is.na(full_model) & is_duplicate == FALSE) %>% 
  rowwise() %>% 
  transmute(Measure = measure_code,
            Reference = paste0("@", source),
            Variable = tools::toTitleCase(subscale_code),
            Variable = str_replace_all(Variable, "_", " "),
            n = nrow(eval(parse(text = measure_code))),
            r = round(latent_cor(data = eval(parse(text = measure_code)), 
                                 short_model = short_model, 
                                 full_model = full_model)$estimate, digits = 2),
            `95%_CI` = paste0(
              round(
                latent_cor(
                  data = eval(parse(text = measure_code)), 
                  short_model = short_model, 
                  full_model = full_model)$conf.int[1], digits = 2), ",",
              round(
                latent_cor(data = eval(parse(text = measure_code)),
                           short_model = short_model, 
                           full_model = full_model)$conf.int[2], digits = 2)),
            `$\\omega$ full` = round(
              latent_cor_comp_rel(data = eval(parse(text = measure_code)),
                                  model = full_model), digits = 2), 
            `$\\omega$ short` = round(
              latent_cor_comp_rel(data = eval(parse(text = measure_code)),
                                  model = short_model), digits = 2)) %>% 
  kable(booktabs = TRUE, 
        caption = "Latent variable correlations between full- and short-form measures.") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE) %>% 
  collapse_rows(c(1:2,4), valign = "top")

```

> (Need to add PESS data if available - UK Sport/Dave Markland?)

* Validity
  * Variables with validity evidence from other studies `r nrow(filter(eshot_vars, short_available == TRUE))`
  * Variables with validity evidence from this study `r nrow(eshot_vars %>% filter(!is.na(full_model) & is_duplicate == FALSE)) + nrow(filter(eshot_vars, is_duplicate == TRUE))`
  * Variables that were self-efficacy items constructed specifically for this project `r sum(str_detect(tolower(eshot_vars$var_label), "efficacy")) - 3 + sum(str_detect(tolower(eshot_vars$var_label), "discrep"))`
  * Variables available on CMS `r sum(str_detect(tolower(eshot_vars$item_wording), "cms"), na.rm = TRUE)`
  * Variables that are total scores `r sum(eshot_vars$is_total == TRUE, na.rm = TRUE)`
  * **WHICH variables don't have any validity evidence??**

* FMPS correlations are high, but the internal consistency measure is low
    * Was this because we wanted to keep items in that captured part of the construct that other bits didn't?

Using the process described above, `r sum(eshot_vars$n_items_full, na.rm = TRUE) - sum(eshot_vars$n_items_short, na.rm = TRUE) %>% round()` items were removed, assuming 7 seconds per item [@Qualtrics], this equates to a survey that is approximately `r round(((sum(eshot_vars$n_items_full, na.rm = TRUE) - sum(eshot_vars$n_items_short, na.rm = TRUE))*7)/60)` minutes shorter. 


### Discussion

Study 1 identified suitable short measures for each construct of interest. Whilst this study significantly reduced the number of items that participants would be required to answer, there were still too many to ask a single participant to answer. Including all of the items identified thus far would require candidates to spend approximately `r round((sum(eshot_vars$n_items_short, na.rm = TRUE)*7)/60)` minutes answering questions; participants would also be required to read the information sheet, transition between pages, etc. Therefore, Study 2 sought to reduce the number of constructs, by identifying those which were not important discriminatory variables.


## Study 2 - Item reduction

Having selected appropriate items for the variables we wanted to collect data for, we were left with an item pool of `r sum(feature_retention$nItems) %>% round()`, which was deemed too many to ask any individual participant as most would not complete the survey and those who did, would probably not be representative of the population. Therefore, we created four surveys, each of which contained a subset of the variables that we wanted to collect data for. Each variable was included in at least two of the surveys and each pairwise combination of variables was included in at least one survey. This was done to both collect as much data as possible and also to ensure that two-way interactions could be explored.

Figure \@ref(fig:survey-tool-overlap) shows a visual representation of the distribution of constructs between the four groups, a full list of variables and the groups that they were asked in can be found in **REF**.


> (Need to edit image to make it more simple.)
> (Have I explained the time point thing higher up?)


```{r survey-tool-overlap, fig.cap="Survey tool overlap"}
include_graphics("plots/SurveyToolOverlap.png")
```


### Method

#### Participants 

In November 2018, we contacted all candidates trained between 2008 and 2016 ($n =$ `r 948 + 950 + 949 + 947`). Each candidate had been randomly assigned to one of four groups (stratified by year of training) using the `randomizr` package [@R-randomizr] and candidates from each of these groups were invited to complete one of the surveys described above using the Qualtrics survey platform [@Qualtrics]. Complete responses were collected from `r nrow(Group1to4)` participants (Table \@ref(tab:study-2-participants)).


```{r study-2-participants, results = "asis"}
Group1to4 %>% 
  mutate(Sex = case_when(SexId == 1 ~ "Male",
                         SexId == 2 ~ "Female"),
         Group = str_replace_all(Group, "Group", "")) %>% 
  group_by(Group) %>% 
  summarise(n = n(),
            `% female` = mean(if_else(SexId == 2, 1 ,0) * 100),
            age = mean(v142, na.rm = TRUE),
            time = mean(interval(firstMLTrainingDate, EndDate)/years(1), 
            na.rm = TRUE)) %>% 
  kable(caption = "Survey participants per group", digits = 2, booktab = TRUE) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
```


Once data collection was complete, each of the four groups was then split in two, one group for those candidates who had DLOG data and one group who did not have DLOG data. This was done as the pattern recognition procedure cannot handle missing data and we would then have had to omit all DLOG data, which would have left us unable to identify interactions between the survey and experience data. Once the groups had been split into these two groups, we created two data sets within each one for those who did not and then for each classification problem. This resulted in the following data sets for each survey group:

  1) Getting to assessment within 18 months of training - no DLOG data
  2) Getting to assessment within 18 months of training - with DLOG data
  3) Passing the first assessment - no DLOG data
  4) Passing the first assessment - with DLOG data

Figure \@ref(fig:study2-participant-sankey) provides a visual representation of the groups described above.

> (need to manually position nodes)


```{r study2-participant-sankey, fig.cap="Study 2 participants."}
plot_ly(
    type = "sankey",
    orientation = "h",

    node = list(
      label = c("Completed survey", "DLOG", "No DLOG", 
                "GTA_Group 1", "GTA_Group 2", "GTA_Group 3", "GTA_Group 4", # GTA DLOG groups
                "GTA_Group 1", "GTA_Group 2", "GTA_Group 3", "GTA_Group 4", # GTA No DLOG groups
                "Not assessed",
                "FTP_Group 1", "FTP_Group 2", "FTP_Group 3", "FTP_Group 4", # FTP DLOG groups
                "FTP_Group 1", "FTP_Group 2", "FTP_Group 3", "FTP_Group 4", # FTP No DLOG groups
                "Passed", "Did not pass"),
      color = c("blue", "blue", "blue", 
                rep(c("gold", "darkcyan", "blueviolet", "deeppink"), 2),
                "blue",
                rep(c("gold", "darkcyan", "blueviolet", "deeppink"), 2),
                "green", "red"),
      pad = 15,
      thickness = 20,
      line = list(
        color = "black",
        width = 0.5
      )
    ),

    link = list(
      # node that data originates from
      source = c(
        # DLOG or non-DLOG
        0,0,
        # Split into 4 DLOG and 4 non-dlog groups
                 rep(1, 4),
                 rep(2, 4),
        # Not assessed candidates
                 3:10, 
        # Assessed groups
                 3:10
        ), 
      
      # node that data is going to
      target = c(
        # DLOG or non-DLOG
                 1,2,
        # Split into 4 DLOG and 4 non-dlog groups
                 3:6,
                 7:10,
        # Not assessed candidates
                 rep(11, 8),
        # Assessed groups
                 12:19), 
      
      # number of cases being transferred
      value =  c(
        # DLOG or non-DLOG
                 nrow(filter(Group1to4, LogbookType1 != 2)),
                 nrow(filter(Group1to4, LogbookType1 == 2)),
        # Split into 4 groups for DLOG then non DLOG
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group1")),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group2")),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group3")),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group4")),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group1")),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group2")),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group3")),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group4")),
          # Not-assessed
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group1" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group2" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group3" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group4" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group1" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group2" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group3" &
                               is.na(FirstTimePass) == TRUE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group4" &
                               is.na(FirstTimePass) == TRUE)),
        # Assessed
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group1" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group2" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group3" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 != 2 &
                               Group == "Group4" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group1" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group2" &
                               is.na(FirstTimePass) == FALSE)),
                 nrow(filter(Group1to4, LogbookType1 == 2 &
                               Group == "Group3" &
                               is.na(FirstTimePass) == FALSE)),
        nrow(filter(Group1to4, LogbookType1 == 2 &
                      Group == "Group4" &
                      is.na(FirstTimePass) == FALSE))
        
      )  
    )
) %>% 
  layout(
    font = list(
      size = 10
    )
  ) -> p

if (knitr::is_html_output()) {
  p
} else
{
  p %>%
    orca(file = "plots/study2-participant-sankey.png")
  
  include_graphics("plots/study2-participant-sankey.png",
                   auto_pdf = TRUE)
}

```


In our data (and the population), most candidates have not been assessed 18 months after their training course. To ensure an orthogonal design (i.e., both groups were of equal size) we selected a random sample of candidates who had not been assessed 18 months after their training course of equal size to the group of candidates who had been assessed.

>(Could/should I do something to check the representativeness of the samples (e.g., using sex, age, board)? Equivilance testing?)


#### Analytical method - pattern recognition

We used pattern recognition analyses to identify the most important discriminatory variables within each group; by identifying the most important, we were able to infer which variables were not important discriminatory variables. Pattern recognition analyses, originally developed in bioinformatics [@Duda2000], use machine learning algorithms to identify a set of discriminatory features (variables), which can be used to identify the class of an object. Pattern recognition analysis is more appropriate for these data than "traditional" methods (e.g., discriminant function analyses) as they employ both linear and non-linear functions and therefore reflect multiple and complex interactions and not just "main-effects". Pattern recognition analyses have also been used in a number of recent studies to examine differences between athletes of different performance levels [e.g., @Gullich2019; @Jones2019a].

More specifically, we used a pattern recognition procedure that has been developed for analysing what are known as *short and wide* data sets [i.e., data sets that contain more variables than cases; @Jones2017a]. This procedure is a three-part process. First, we aimed to identify a set of features which correlated well with the class but had a low correlation with one another (feature selection). Then we tested the ability of this feature subset to correctly classify the candidates according to the criterion variable for that analysis (classification). Finally, we refined the feature subset to identify the simplest solution that best explained the data (recursive feature elimination). 

We carried out the pattern recognition procedure outlined above twice for each of the four pilot surveys. The first set of analyses identified the most important features for discriminating candidates who get to assessment within 18 months of their training from those who do not. The second set of analyses identified the features which best discriminated candidates who passed their first assessment from those who did not.

Analyses were carried out using WEKA 3-9-3 open source software issued under the GNU General Public License version 3 [@Bouckaert2018; @Frank2016]. WEKA is a machine learning workbench with a collection of algorithms widely used for data mining, machine learning, and pattern recognition.

>(Something about using ensembles [@Bolon-Canedo2012; @Bolon-Canedo2014] and also combination of supervised and unsupervised
> Is this still distributed feature selection but horizontally?
> Difference between individual and subset evaluation methods
> Tables 1 & 2 in Bolón-Canedo et al [-@Bolon-Canedo2015] have info about 3 of the 4 FS methods we use and their strengths & weaknesses)


##### Pre-processing

For feature selection and model identification, it is important that good quality data is used and that there are no missing values **REF**. Where there was missing data, we had a choice, remove the case or remove the variable^[Imputing values would not be appropriate for these data as this was usually due to candidate: (a) not having completed the survey, (b) having had an atypical journey through the pathway (e.g., received an exemption from training), or (c) provided an inappropriate answer for a question.]. The choice of removing the case or variable was decided by the amount of missing data within the variable in an effort to use as much data as possible. Where there were few cases with missing data for that variable, we removed the cases and where there were many cases with missing data for that variable, we removed the variable. Once each of the 16 data sets had been cleaned, the data were standardised within sex using the `mousetrap` package [@R-mousetrap], to control for sex differences^[We considered exploring the data having standardised them by training provider, however, we felt that the variance in number of candidates trained by each provider was too high.].

> (How many variables and cases were excluded and what was the rationale?)


##### Feature Selection

Best practice guidelines for short and wide data recommend that feature selection is carried out using a number of different methods [@Jones2017a]. With this in mind we used four feature selection algorithms: Correlation Feature Subset with a Best First Evaluator [CFS; @Hall1999], Correlation Attribute Evaluator [CAE; @Bouckaert2018], Relief-f [@Kira1992], and Support Vector Machine - Recursive Feature Elimination [SVM-RFE; @Guyon2002]. CAE, Relief-f, and SVM-RFE rank all variables in order of merit (magnitude of relationship), whereas CFS selects a subset of features that are highly correlated with the class but not with one another. As only CFS provides a subset of features, the top 20 features were selected from the rankings provided by the other three algorithms (as long as the attribute merit was greater than zero).

These analyses were carried out in a centralised fashion, with all variables being included at the same time [@Bolon-Canedo2015a]. We employed a *leave-one-out cross-validation* (LOO-CV) protocol, a special case of $K$-fold cross-validation, where $K = N$, as it reduces the impact of each object on the feature selection process by increasing the generalisability of the model [@Hastie2009]. Each data set was split into $K$ parts or *folds*, with each part having an approximately equal number of cases. The $K$th fold is then removed from the data and the feature selection algorithm is then applied to the remaining data, with each feature being assigned a merit score (or being selected/not for CFS), once this has been repeated $K$ times the merit score for each attribute is averaged across the $K$ iterations.

All of these are well established feature selection methods and the most important point to note about these four methods is that each works in very a different way. Therefore, the greater the number of algorithms which select a feature, the more confident one can be that it is important as it is less likely that the feature has been chosen by chance [@Visa2011]. For each of the 16 data sets this process yielded a number feature subsets, one containing those features which had been selected by at least two feature selection algorithms (2s), another containing those features which had been selected by at least three feature selection algorithms (3s), and where possible a third subset where features has been selected by all four algorithms (4s).


##### Initial Classification

For each of the feature subsets, we carried out *initial classification* experiments. Each of these experiments used four different classification algorithms with their default settings and LOO-CV: Naïve Bayes [NB; @John1995], Sequential Minimal Optimization [SMO; @Platt1998], Instance Based Learning [IBk; @Aha1991], J48 Decision Tree [J48; @Quinlan1993]. As with feature selection, the more consistent the results from each algorithm (classification accuracy) for a feature subset, the more confidence we can place in the predictive validity of that subset.

An important point to note that as all of the data have been seen during the feature selection stage the classification rates may be slightly higher than they would be for previously unseen data  [@Kuncheva2018; @Smialowski2010]. However, in this instance, the aim is not to create a predictive model, but to identify which features should be retained to create a survey tool and classification rates are only being used to compare the predictive power of feature subsets from the same data set - any inflation of classification rates will be shared by the feature subsets under consideration.


##### Final Classification

Having conducted the initial classification experiments, we sought to identify more parsimonious models, potentially with higher classification accuracies using the Recursive Elimination Method [@Guyon2002]. To do so we took each feature subset with more than five features in, examined the normalised SMO weight provided by the SMO classifier and removed the feature with the lowest weight before re-running the four classifiers on the, now smaller, feature subset. This continued in an iterative fashion until all features with a SMO weight > .3 had been removed, the iteration with the best classification rate was then retained.


### Results

#### Classification rates

In 15 of the 16 datasets we were able identify a feature subset which could be used to correctly classify candidates with at least *good* accuracy (i.e., at least one classifier for that data set had a classification rate over 70%). For the other data set we were able to identify a feature subset which could be used to classify candidates with *moderate* accuracy (i.e., at least one classifier for that data set had a classification rate over 60%, Table \@ref(tab:study-2-classification-rates)).

Having identified a number of feature subsets that could be used to classify candidates in each group, we identified the items which we wanted to retain for the final survey. As not all items were asked to the same number of groups, we scored each item by the number of times that it was selected divided by the number of times it was asked. This was done so that the item retention process was not biased by the number of times that an item was asked. Items were retained if they were selected for the best models in at least half of the datasets they were asked to.

Of the `r nrow(feature_retention)` variables, `r nrow(feature_retention) - sum(feature_retention$RetainedEither)` variables were retained based on the criteria above. Some of these variables were sum totals of constructs including variables not selected, therefore we chose to retain a further `r sum(!is.na(feature_retention$EitherAdditional))` variables. This process resulted in `r sum(feature_retention$EitherAdditional, na.rm = TRUE) + sum(feature_retention$nItemsEither, na.rm = TRUE) %>% round(.,1)` items being retained for the final survey (see **APPENDIX** for a list of the variables retained for the final survey).


### Discussion

This study sought to create a survey tool which could be administered to candidates who had completed an ML training course in order to help us identify the most important discriminatory variables for candidates who: (a) did or did not get to an assessment within 18 months of their training course and (b) did or did not pass their first assessment. Whilst no single candidate provided data for all of the variables, we were able to discriminate candidates with a degree of accuracy substantially greater than chance. The findings shows that firstly the measures used in the survey work, and secondly, that we collected data about variables which explain some of the variance in the criterion variables. It is important to note that just because a construct has not been selected, it is not necissarialy unimportant. Variables not selected as discriminatory variables may infact be important commonalities between the groups. The next study collected data from candidates who attended an ML training course between 2016 and 2018 on all of the variables retained following this study.

Including DLOG data in the models did not appear to improve the classification rates in any substantive way. This suggests that the variance explained by those data is better explained by survey variables. A likely explanation for this is that candidates use the DLOGs in different ways. Some candidates will log every experience that they have, some will log only the best of their experiences, some will log only their relevant experience, and some will log only the experience they need to meet the pre-requites for the course (potentially from an extremely large pool of experience). The use of DLOG in these different ways creates messy data, with no easy way to distinguish a candidate who only has 40 QMDs and a candidate who has far more than that but only logs 40 as they do not feel it would benefit them to log more.


> (Ross: This implies to me that if there were two items to measure a construct (for example) you put each item and the sum total of the construct into the analysis. If this is what you did (which I am pretty sure it is), I think its important that earlier you make this point clear and explain why it is appropriate/good.)

