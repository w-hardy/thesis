---
output: bookdown::word_document2
---

# Key Discriminatory Factors

```{r 03-setup, include=FALSE}
library(plyr)
library(tidyverse)
library(lubridate)
library(readxl)
library(knitr)
library(kableExtra)
library(ggthemes)
library(plotly)
library(randomizr)
library(data.table)
library(lavaan)
library(survival)
library(survminer)
library(papaja)
library(tidytidbits)
library(ggExtra)



knitr::opts_chunk$set(echo = FALSE, cache = FALSE, warning = FALSE, 
                      message = FALSE)
doc.type <- knitr::opts_knit$get('rmarkdown.pandoc.to')

########################################
################# DATA #################
########################################
feature_retention <- 
  read_xlsx("../3 eshot/PRA_Results/FeatureRetention.xlsx", 
            sheet = "Sheet1")[1:150,]

Group1to4 <- readRDS("chapter_3_data/Group1to4.rds")

group_5 <- readRDS("chapter_3_data/Group5.rds")
group_5_gta = read_csv("../3 eshot/Data/Group5_GTA_v3.csv") %>% 
    filter(!CandidateId %in% 
           c("188254", "292087","130036","184996", "258131", "193950", "112773",
             "188235") #inappropriate response to personal projects
         & is.na(RegExpTimeToAssess) == FALSE)
group_5_pra <- group_5 %>% 
  filter(CandidateId %in% group_5_gta$CandidateId &
           is.na(t12_mtn_percent_1) == FALSE)


Group5_FTP = read_csv("../3 eshot/Data/Group5_FTP_v3.csv")
G5_female_GTA_18m_comb <- read_csv("../3 eshot/Data/G5_female_GTA_18m_comb.csv")
G5_female_GTA_18m_train <- read_csv("../3 eshot/Data/G5_female_GTA_18m_train.csv")
G5_male_GTA_18m_train <- read_csv("../3 eshot/Data/G5_male_GTA_18m_train.csv")
G5_male_GTA_18m_test <- read_csv("../3 eshot/Data/G5_male_GTA_18m_test.csv")
G5_male_GTA_18m_pred <- read_csv("../3 eshot/Data/G5_male_GTA_18m_pred.csv")
g5_zsex_ftp_consol <- read_csv("../3 eshot/Data/G5_FTP_consol_v3.csv")


### PRA results ###
#### Feature subsets ####
g5_female_gta_18m_fs <- 
  read_csv("../3 eshot/PRA_Results/v1/gta_female_g5_18m_FS.csv", 
           col_types = "cci")
g5_male_gta_18m_fs <- 
  read_excel("../3 eshot/PRA_Results/v1/gta_male_g5_18m_FS.xlsx")

g5_zsex_ftp_fs <- 
  read_csv("../3 eshot/PRA_Results/v3/ftp_g5_18m_FS.csv", 
           col_types = "cci")

#### Classification rates ####
G5_female_GTA18m_PRA <- 
  # read_xlsx("../3 eshot/PRA_Results/v3/g5_female_gta18m_comb_classification.xlsx", 
  #           sheet = "Sheet1", range = "A10:J38")  %>% 
  read_xlsx("../3 eshot/PRA_Results/v1/G5_female_GTA_18m_comb.xlsx",
            sheet = "Classification (2)") %>% 
  left_join(g5_female_gta_18m_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median)


G5_male_GTA18m_train_PRA <- 
  # read_xlsx("../3 eshot/PRA_Results/v3/g5_male_gta18m_train_classification.xlsx", 
  #           sheet = "Sheet1", range = "A10:L38") %>% 
  read_xlsx("../3 eshot/PRA_Results/v1/G5_male_GTA_18m_test_n10.xlsx", 
            sheet = "Classification") %>% 
  left_join(g5_male_gta_18m_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median) %>% 
  select(-ends_with("4s"))

G5_zsex_ftp_train_PRA <- 
  read_xlsx("../3 eshot/PRA_Results/v3/g5_zsex_consol_ftp_classification.xlsx", 
            sheet = "Sheet1", range = "A10:J36") %>% 
  left_join(g5_zsex_ftp_fs, 
            by = c("Dataset" = "feature_subset")) %>% 
  select(Dataset, n, NB, SMO, IBk, J48, mean, median)

G5_male_GTA18m_valid_PRA <- 
  read_excel("../3 eshot/PRA_Results/v3/g5_male_gta18m_validation_classification.xlsx")

#### PRA predictions ####
g5_male_GTA_18m_pred_perform <- 
  read_rds("../3 eshot/PRA_Results/v1/g5_male_gta_18m_pred/g5_male_GTA_18m_pred_perform.rds")


#### Variable names ####
eshot_vars <- 
  read_xlsx("../3 eshot/Design/eshot_variables.xlsx", 
            sheet = "Sheet1", na = "NA") %>%
  mutate(full_model = full_model %>% str_remove_all("\r"),
    short_model = short_model %>% str_remove_all("\r"))

dlog_vars <- read_xlsx("../3 eshot/Design/dlog_variables.xlsx")

var_descriptions <- 
  bind_rows(eshot_vars, dlog_vars)

################# FUNCTIONS #################
normalize <- 
  function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
  }

##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- 
  function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
           conf.interval=.95, .drop=TRUE) {
    

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- 
      function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
      }
    
    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- plyr::ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- plyr::rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
  }

n_pct_col <- function(n, pct, digits = 2){
  return(paste0(n, " (", round(pct, digits), if(is_latex_output()){"\\%)"} else {"%)"}))
}

my_table <- function(data = ., caption = "ADD CAPTION", digits = 2){
  kable(x = data, caption = caption, digits = digits, booktab = TRUE,
        format = if (is_latex_output()) {"latex"} else {"html"}) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  latex_options = "HOLD_position", position = "center",
                  full_width = FALSE)
}

numbers2words <- function(x){
#https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r
  ## Function by John Fox found here: 
  ## http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html
  ## Tweaks by AJH to add commas and "and"
  helper <- function(x){
    
    digits <- rev(strsplit(as.character(x), "")[[1]])
    nDigits <- length(digits)
    if (nDigits == 1) as.vector(ones[digits])
    else if (nDigits == 2)
      if (x <= 19) as.vector(teens[digits[1]])
    else trim(paste(tens[digits[2]],
                    Recall(as.numeric(digits[1]))))
    else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred and", 
                                      Recall(makeNumber(digits[2:1]))))
    else {
      nSuffix <- ((nDigits + 2) %/% 3) - 1
      if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
      trim(paste(Recall(makeNumber(digits[
        nDigits:(3*nSuffix + 1)])),
        suffixes[nSuffix],"," ,
        Recall(makeNumber(digits[(3*nSuffix):1]))))
    }
  }
  trim <- function(text){
    #Tidy leading/trailing whitespace, space before comma
    text=gsub("^\ ", "", gsub("\ *$", "", gsub("\ ,",",",text)))
    #Clear any trailing " and"
    text=gsub(" and$","",text)
    #Clear any trailing comma
    gsub("\ *,$","",text)
  }  
  makeNumber <- function(...) as.numeric(paste(..., collapse=""))     
  #Disable scientific notation
  opts <- options(scipen=100) 
  on.exit(options(opts)) 
  ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
            "eight", "nine") 
  names(ones) <- 0:9 
  teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
             "sixteen", " seventeen", "eighteen", "nineteen")
  names(teens) <- 0:9 
  tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty",
            "ninety") 
  names(tens) <- 2:9 
  x <- round(x)
  suffixes <- c("thousand", "million", "billion", "trillion")     
  if (length(x) > 1) return(trim(sapply(x, helper)))
  helper(x)
}

if (is_latex_output()) {font = "serif"} else {font = "sans"}

```


## Introduction {#ml-pra-introduction}

From the results of [Chapter 2](#ml-qualitative) it was clear that there was no single factor that determined whether or not a candidate would complete the Mountain Leader qualification. Instead the results suggested that both the main effects of, and interactions between, a myriad of factors were important. The study in Chapter 2 collected data from those who were involved with the organisation and delivery of Mountain Leader training and assessment courses. To our knowledge, Chapter 2 represents the most in depth investigation of factors influencing the completion of the Mountain Leader; however, it does not include any data from candidates themselves. Therefore, to develop a more holistic view of the factors influencing the completion of the Mountain Leader qualification, this chapter reports the findings of a study that collected data for these variables from candidates who had registered and attended a training course for the Mountain Leader qualification.


### Pathways to Expertise

The development of expertise and the pathways to it are of interest in a variety of domains. Historically, most studies have examined the impacts of specific factors on the completion of a training pathway [e.g., delays in completing PhDs; @VandeSchoot2013a]. However, these approaches fail to acknowledge that there may be vast differences for individuals in their pathway to expertise. As such, a growing number of researchers now recommend that multi-disciplinary approaches should be adopted for identifying the potentially complex interactions that influence talent/expertise development [e.g., @Abernethy2013; @Gullich2019; @Johnston2018; @Pearson2006; @Rees2016; @Schorer2013].

Developing expertise is likely to be the result of complex interactions between a variety of developmental factors [@Baker2013; @Gagne2004; @Johnston2018]. Further, different factors will be more salient at different points of development pathways [@Rees2016]. Based on these principles, a number of projects in the elite sport domain have recently begun to explore the most important factors in the development of athletes in elite pathways. Examples of projects that have sought to identify the most important discriminatory variables for pathways are: The Great British Medallist Project [GBM; @Gullich2019; @Hardy2017; @Rees2016]; "Game Changers" Discriminating Features within the Microstructure of Practice and Developmental Histories of Super-Elite Cricketers [@Jones2019a; @Jones2019b; @Jones2020]. Both projects used a mixed-methods approach, collecting both rich qualitative data from athletes and making use of state-of-the-art machine learning techniques to identify sets of variables, whose main-effects and interactive-effects were able to discriminate between athletes at different performance levels.


### The current study

From the results of the [Chapter 2](#ml-qualitative) and a workshop with the Mountain Training UKI council, we identified `r nrow(eshot_vars)` variables that were deemed as potentially important to the completion of the Mountain Leader qualification. For the sake of brevity, we have not included a description of all the relevant theory for these variables here. Instead, we have included an introduction of relevant theory at the start of the discussion.

Given the large number of potentially important variables, it was necessary to carry out extensive pilot work to identify suitable short-form measures of constructs and reduce the number of constructs included in a survey tool, so that it could be administered to candidates without being unduly onerous to complete. To improve the readability of this chapter, this pilot work is reported in [Appendix A](#survey-tool-dev). We encourage the reader to engage with this material having read this chapter, to understand the rigour of the pilot work and the techniques employed to reduce the number of items required to measure the constructs of interest, a method which may prove useful in other domains.

The aim of the present study was to identify variables influencing completion of the Mountain Leader qualification. In the preceding studies, we identified potentially important variables and created a survey tool of reasonable length to collect data for these variables. In this study, we used that survey tool to collect data from candidates in order to identify important discriminatory variables for each of the following classification problems:

1. Male candidates who are assessed within 18 months of their training from those who are not.
2. Female candidates who are assessed within 18 months of their training from those who are not.
3. Candidates who pass their first assessment from those who do not.

This results for this study are therefore presented in three sections, one for each of the classification problems above. This is then followed by a discussion of each set of results, couched in relevant theory.


## Method {#ml-pra-method}

### Participants {#ml-pra-participants}

```{r group-5-demo, include = FALSE}
group_5_pra %>% 
  filter(CandidateId %in% group_5_gta$CandidateId &
           is.na(t12_mtn_percent_1) == FALSE) %>% 
  group_by(SexId) %>% 
  summarise(survey_age_mean = mean(interval(DOB, SurveyEndDate)/years(), na.rm = TRUE),
            survey_age_sd = sd(interval(DOB, SurveyEndDate)/years(), na.rm = TRUE)) %>% 
  round(2) -> group_5_demo
```


We contacted all candidates who attended their first Mountain Leader training course in 2017 or 2018, inviting them to participate in the study; 1,030 candidates started the survey and `r length(na.omit(group_5_pra$SelfEfficacyDiscrepancy1))` completed the survey (`r round((length(na.omit(group_5_pra$SelfEfficacyDiscrepancy1))/2867)*100,2)`% response rate). Table \@ref(tab:group-5-pra-descriptives) provides a summary of demographic variables for this sample. These candidates had been trained by `r group_5_pra$firstMLTrainingProviderId %>% unique() %>% length()` different providers and those who had been assessed, had been assessed by `r group_5_pra$firstMLAssessmentProvId %>% unique() %>% length()` different providers.


```{r group-5-pra-descriptives}
group_5_pra %>% 
  mutate(Sex = case_when(SexId == 2 ~ "Female",
                         SexId == 1 ~ "Male")) %>% 
  filter(is.na(SelfEfficacyDiscrepancy1) == FALSE) %>% 
  group_by(Sex) %>% 
  summarise(n = n(),
            Age = mean(TrainAge, na.rm = TRUE),
            age_sd = sd(TrainAge, na.rm = TRUE),
            white_n = sum(IsWhite, na.rm = TRUE),
            `White` = mean(IsWhite)*100,
            assessed_18m_n = sum(Assessed_18months),
            assessed_n = sum(Assessed),
            passed_n = sum(FirstTimePass, na.rm = TRUE),
            `Assessed within 18 months` = 
              (sum(Assessed_18months, na.rm = TRUE)/n)*100,
            `Assessed` = round((sum(Assessed)/n)*100,1),
            `Passed first time` = 
              (sum(FirstTimePass, na.rm = TRUE)/assessed_n)*100
            ) %>% 
  transmute(Sex = Sex,
            n = n,
            Age = round(Age, 2),
            age_sd = round(age_sd, 2),
            White = n_pct_col(white_n, White),
            Assessed = n_pct_col(assessed_n, Assessed),
            `Assessed within 18 months` = n_pct_col(assessed_18m_n, `Assessed within 18 months`),
            `Passed first time` = n_pct_col(passed_n, `Passed first time`)) %>% 
  kable(caption = "Participant descriptive statistics.", escape = FALSE, 
        col.names = if(is_latex_output())
        {c("Sex", "n", "M\\textsubscript{age}", "SD\\textsubscript{age}", "White", "Assessed",
             "Assessed within 18 months", "Passed First Time")}
        else {c("Sex", "n", "M~age~", "SD~age~", "White", "Assessed",
                "Assessed within 18 months", "Passed First Time")},
        booktab = TRUE,
        format = if (is_latex_output()) {"latex"} else {"html"}) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  latex_options = "HOLD_position", position = "center",
                  full_width = FALSE) %>% 
  column_spec(3, width = "1.5cm") %>% 
  column_spec(4, width = "1.5cm") %>% 
  column_spec(5, width = "1.5cm") %>% 
  column_spec(6, width = "1.5cm") %>% 
  column_spec(7, width = "1.5cm") %>% 
  column_spec(8, width = "1.5cm") #%>% 

  #add_footnote(notation = "none", label = "")

```


When responding to the survey, candidates were at different stages of the pathway. Some candidates had been assessed whereas others had not. In addition, some candidates had completed their training course at least 18 months before responding to the survey, and the remainder responded to the survey less than 18 months after their training course. Therefore, candidates could have completed the survey either prospectively or retrospectively with regards to *both* the event (being assessed) and the criterion variable (getting to assessment within 18 months of training) *separately*. As such we were able to create four groups within each sex (see Table \@ref(tab:g5-candidate-survey-time) for descriptive data).

In this study we analysed the data from female and male candidates separately for getting to assessment within 18 months of training as the completion rates for each group are different^[This is a minor departure from the pilot work, where we split candidates according to whether or not they had DLOG data, which meant that it was not possible to then split each of these groups by gender as well because some groups would have been too small to analyse.] but analysed the data from the two groups together for the passing first time analyses as the pass rates are similar. Each of the three analyses had different inclusion criteria and subsequently, used a different subset of candidates. Details of the candidates included in each data set are presented in the sections below and a visual representation of groups that candidates were included in is presented in Figure \@ref(fig:study3-participant-sankey).


```{r g5-candidate-survey-time}
g5_candidate_survey_time <- 
  group_5_pra %>% 
  filter(is.na(SelfEfficacyDiscrepancy1) == FALSE,
         is.na(t12_mtn_percent_1) == FALSE) %>% 
  mutate(Assessment = 
           case_when(SurveyEndDate < firstMLAssessmentDate ~ "Pre-",
                     SurveyEndDate >= firstMLAssessmentDate ~ "Post-",
                     is.na(firstMLAssessmentDate) == TRUE ~ "Pre-"),
         `18 months post-training` = 
           case_when(TrainToSurvey >= 1.5 ~ "Post-",
                     TrainToSurvey < 1.5 ~ "Pre-"),
         gta_18m = `18 months post-training`,
         Sex = case_when(SexId == 2 ~ "Female",
                         SexId == 1 ~ "Male"))

g5_candidate_survey_time %>% 
  group_by(Sex, Assessment, `18 months post-training`) %>% 
  count() %>% 
  janitor::adorn_totals("row") %>% 
  kable(caption = "Candidates pathway progress when completing the survey.", 
        booktab = TRUE, 
        format = if (is_latex_output()) {"latex"} else {"html"}) %>%
  column_spec(3, width = "2cm") %>% 
  collapse_rows(valign = "top", columns = c(1:3)) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                  full_width = FALSE)
```


#### Getting to Assessment Within 18 Months of Training - Male Candidates.

There were `r g5_candidate_survey_time %>% filter(Sex == "Male" & gta_18m == "Post-") %>% nrow()` responses from male candidates who completed the survey more than 18 months after their training course (i.e., retrospectively), `r g5_candidate_survey_time %>% filter(Sex == "Male" & gta_18m == "Post-" & Assessed_18months == 0 & Assessed == 1) %>% nrow()` of whom had been assessed more than 18 months post-training^[These candidates were not included in the analyses as the wording of the questions they answered meant that their responses were not comparable to those who had been assessed within 18 months and those who had not been assessed.], `r g5_candidate_survey_time %>% filter(Sex == "Male" & gta_18m == "Post-" & Assessed_18months == 1) %>% nrow()` of whom had been assessed within 18 months of their training course and `r g5_candidate_survey_time %>% filter(Sex == "Male" & gta_18m == "Post-" & Assessed_18months == 0 & Assessed == 0) %>% nrow()` who not been assessed at the time of completing the survey^[Candidates who had not been assessed within 18 months of their training course but had been assessed prior to completing the survey were excluded from the analysis as the wording of the questions shown to them meant they would not be comparable to the other candidates.]. Therefore, we were able to create a set of *training data* ($n$ = `r nrow(G5_male_GTA_18m_train)`), which we could use to select variables and a set of *test data* ($n$ = `r nrow(G5_male_GTA_18m_test)`), with an equal split of candidates who had and had not been assessed 18 months after their training course. 

In addition to this, `r nrow(G5_male_GTA_18m_pred)` male candidates completed the survey less than 18 months after their training but at the time of writing were at least 18 months post-training. These candidates formed the *validation data* set.


#### Getting to Assessment Within 18 Months of Training - Female Candidates.

```{r gta_f_desc, include=FALSE}
G5_female_GTA_18m_comb %>% 
  left_join(select(group_5, c(CandidateId, TrainToSurvey)), 
            by = "CandidateId") %>% 
  group_by(Assessed_18months) %>% 
  summarise(age = mean(TrainAge),
            sd_age = sd(TrainAge)) %>% 
  round(2) -> G5_gta_f_age

G5_female_GTA_18m_train %>% 
  left_join(select(group_5, c(CandidateId, TrainToSurvey)), 
            by = "CandidateId") %>% 
  group_by(if_else(TrainToSurvey > 1.5, "retro", "pros"), 
           Assessed_18months) %>% 
  count()

# removed from para below ($M$~age~ = `r G5_gta_f_age %>% filter(Assessed_18months == 1) %>% .$age` ± `r G5_gta_f_age %>% filter(Assessed_18months == 1) %>% .$sd_age` years)
```

The data used for this analysis were collected from 27 candidates who had been assessed 18-months after their training  and 27 who had not. We received fewer responses from female candidates and were therefore unable to have separate test or validation data sets. In each group there were 10 candidates who completed the survey retrospectively (i.e., more than 18 months post-training) and 17 who completed the survey prospectively but had completed their training at least 12 months before (i.e., 12-18 months post-training).


#### Passing First Time.

```{r ftp_desc, include=FALSE}
g5_zsex_ftp_consol %>% 
  left_join(select(group_5, 
                   c(CandidateId, SurveyEndDate, firstMLAssessmentDate)), 
            by = "CandidateId") %>% 
  group_by(FirstTimePass) %>% 
  summarise(age = mean(TrainAge),
            sd_age = sd(TrainAge)) %>% 
  round(2) -> G5_ftp_age

g5_zsex_ftp_consol %>% 
  left_join(select(group_5, 
                   c(CandidateId, SurveyEndDate, firstMLAssessmentDate)), 
            by = "CandidateId") %>% 
  group_by(if_else(firstMLAssessmentDate < SurveyEndDate, "retro", "pros"), FirstTimePass) %>%
  rename(time = `if_else(firstMLAssessmentDate < SurveyEndDate, "retro", "pros")`) %>% 
  count() -> G5_ftp_count
```

The data used for this analysis were collected from `r nrow(g5_zsex_ftp_consol)` candidates, `r G5_ftp_count %>% group_by(time) %>% summarise(total = sum(n)) %>% filter(time == "retro") %>% .$total` of whom had been assessed prior to completing the survey and `r G5_ftp_count %>% group_by(time) %>% summarise(total = sum(n)) %>% filter(time == "pros") %>% .$total` of whom had not been assessed before completing the survey. As with the data in female candidates getting to assessment, we combined the retrospective and prospective data to increase the sample size^[This means that no variables about candidates' experience of assessment were included in the analyses. We have run the analyses on just the retrospective data, which allowed us to include some variables about candidates' experiences of assessment, but none of these variables were selected in the best discriminatory subsets, nor were the classification rates significantly higher.]. Twenty three of the 46 candidates passed their assessment first time. Of the `r G5_ftp_count %>% group_by(FirstTimePass) %>% summarise(total = sum(n)) %>% filter(FirstTimePass == 0) %>% .$total` who did not pass, `r G5_ftp_count %>% filter(time == "pros" & FirstTimePass == 0) %>% .$n` completed the survey prospectively. Two of the 23 candidates who did not pass withdrew from their first assessment, none failed, and the remainder were deferred. Seven of those who were deferred only needed to log additional days.


```{r study3-participant-sankey, fig.cap="Study 3 participants. For simplicity, candidates who have not been assessed have not been added to this figure as a final group, therefore it can be assumed that candidates not progressing from one node to another have not been assessed."}
plot_ly(
    type = "sankey",
    arrangement = "snap",
    orientation = "h",
    valueformat = ".0f",
    node = list(
      label = c("Completed survey", "Female", "Male", 
                "18+ months", "12-18 months", "0-12 months", # female GTA grouping
                "18+ months", "12-18 months", "0-12 months", # male GTA grouping
                "Combined", # female analysis
                "Test", "Training", "Validation", # male analysis groups
                #"Not assessed", 
                "Assessed",
                "Passed", "Did not pass"
                ),
      x = c(0, .225, .225,
            rep(.35, 6),
            rep(.525, 4),
            .75, 1, 1),
      y = c(.5, .2, .75, 
            .35, .2, .1,
            1, .7, .6,
            .29, .9, .8, .7,
            .375, .5, .2),
      color = c("blue", "blue", "blue", 
                "gold", "darkcyan", "blueviolet",
                "gold", "darkcyan", "blueviolet",
                "blue",
                "green", "red", "deeppink",
                "blue", "blue",
                "green", "red"),
      pad = 10,
      thickness = 20,
      line = list(
        color = "black",
        width = 0.5
      )
    ),

    link = list(
      # node that data originates from
      source = c(
        
        # female and male
        0,0,
        
        # split by time since training
        rep(1, 3),
        rep(2, 3),
        
        # female analysis
        3:4, 
        
        # male analysis groups
        rep(6, 2), # test & train
        7, # prospective
        
        # not assessed
        rep(c(5, 8, 9:12), 1),
        
        # outcomes
        rep(13,2)
      ), 
      
      # node that data is going to
      target = c(
        
        # female and male
        1,2,
        
        # split by time since training
        3:5,
        6:8,
        
        # female analysis
        rep(9 , 2),
        
        # male analysis groups
        10, 11,  # test & train
        12, # prospective
        
        # # not assessed
        # rep(13, 6),
        
        # assessed
        rep(13, 6),
        
        #outcome
        14,
        15
      ),
      
      # number of cases being transferred
      value =  c(
        # female and male
        nrow(filter((group_5_pra), SexId == 2)),
        nrow(filter((group_5_pra), SexId == 1)),

        # split by time since training
        nrow(filter((group_5_pra), SexId == 2 & TrainToSurvey >= 1.5)),
        nrow(filter((group_5_pra), SexId == 2 & between(TrainToSurvey, 1, 1.5))),
        nrow(filter((group_5_pra), SexId == 2 & TrainToSurvey < 1)),
        nrow(filter((group_5_pra), SexId == 1 & TrainToSurvey >= 1.5)),
        nrow(filter((group_5_pra), SexId == 1 & between(TrainToSurvey, 1, 1.5))),
        nrow(filter((group_5_pra), SexId == 1 & TrainToSurvey < 1)),
        
        # female analysis
        nrow(filter((group_5_pra), SexId == 2 & TrainToSurvey >= 1.5)),
        nrow(filter((group_5_pra), SexId == 2 & between(TrainToSurvey, 1, 1.5))),
        
        # male analysis groups
        10, # test
        nrow(filter((group_5_pra), SexId == 1 & TrainToSurvey >= 1.5)) - 10, # train
        nrow(filter((group_5_pra), SexId == 1 & between(TrainToSurvey, 1, 1.5))), # prospective
        
        # # not assessed
        # nrow(filter(group_5, SexId == 2 & TrainToSurvey < 1 & Assessed == 0)), # 0-12m female
        # nrow(filter(group_5, SexId == 1 & TrainToSurvey < 1 & Assessed == 0)), # 0-12m male
        # nrow(filter(group_5, SexId == 2 & TrainToSurvey > 1 & Assessed == 0)), # combined
        # 5, # test
        # nrow(filter(group_5, SexId == 1 & TrainToSurvey >= 1.5 & Assessed == 0)), # train
        # nrow(filter(group_5, SexId == 1 & between(TrainToSurvey, 1, 1.5) & Assessed == 0)), # prospective
        
        # assessed
        nrow(filter((group_5_pra), SexId == 2 & TrainToSurvey < 1 & Assessed == 1)), # 0-12m female
        nrow(filter((group_5_pra), SexId == 1 & TrainToSurvey < 1 & Assessed == 1)), # 0-12m male
        nrow(filter((group_5_pra), SexId == 2 & TrainToSurvey >= 1 & Assessed == 1)), # combined
        5, # test
        nrow(filter((group_5_pra), SexId == 1 & TrainToSurvey >= 1.5 & Assessed == 1)) - 5, # train
        nrow(filter((group_5_pra), SexId == 1 & between(TrainToSurvey, 1, 1.5) & Assessed == 1)), # prospective
        
        # outcome
        nrow(filter(group_5_pra, FirstTimePass == 1)), # passed
        nrow(filter(group_5_pra, FirstTimePass == 0)) # did not pass
      )
    )
) %>% 
  layout(
    font = list(
      size = 10
    )
  ) -> p

  p %>%
    orca(file = "plots/study3-participant-sankey.pdf", width = "15cm", parallel_limit = 2)

if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/study3-participant-sankey.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


### Measures

Data for this study were collected from candidates through two mechanisms. Firstly, data were retrieved from Mountain Training’s Candidate Management System (CMS). The CMS data are held by Mountain Training and include information on candidate demographics, training course attendance, and experience data in the form of a digital logbook (DLOG). Secondly, the research team developed a self-report survey tool to collect quantitative data from Mountain Leader candidates that was not already held by Mountain Training. Given the large number of factors identified as important in [the qualitative chapter](#ml-qualitative), the first challenge was to create a survey tool that was of reasonable length and would therefore be completed by candidates. 

The development of the survey tool involved two studies, one to identify suitable short measures for constructs of interest and another to reduce the number of constructs that we needed to collect data from candidates for. We believe that it is important for the reader to understand the work that underpins the survey tool, however, including that detail here would distract from the purpose of this study---understanding which factors best discriminate candidates who are assessed from those who are not. Therefore, the development of this survey tool and a full list of measures included is described in detail in [Appendix A](#survey-tool-dev). The survey tool included the following sections:

1) Personality (e.g., big five, perfectionism).
2) Socio-demographics (e.g., distance from nearest mountains).
3) Intentions and expectations (e.g., intention to be assessed, expected time to assessment).
4) Motivation (e.g., participatory and regulatory motives).
5) Experience of training (e.g., training staff coaching behaviours).
6) Experiences post-training (e.g., life changes).
7) Social support (e.g., emotional, esteem).
8) Self-efficacy (e.g., to carry out specific Mountain Leader related tasks).
9) Experience of assessment if relevant (e.g., provision of a need supportive environment, inter-personal conflict).


### Procedure

After the study received institutional ethical approval, Mountain Training candidates who had attended a Mountain Leader training course in 2017 and 2018 were invited to complete the survey tool through the Qualtrics online survey platform [@Qualtrics]. Before completing the survey, participants were required to provide informed consent. Following this, they were asked to indicate if they had attended a Mountain Leader assessment course or not, so that they were shown the appropriate questions. We then instructed participants to think about *how they felt before their first assessment* for consolidation and assessment related questions if they had been assessed, and to think *how they felt now* when answering these questions if they had not been assessed when completing the survey.


### Analytical Method

We used pattern recognition analyses to identify the most important discriminatory variables within each group. By identifying the most important, we were able to infer which variables were not important discriminatory variables. Pattern recognition analyses, originally developed in bioinformatics [@Duda2000], use machine learning algorithms to identify a set of discriminatory features (variables), which can be used to identify the class (group) of objects (candidates). Pattern recognition analysis is more appropriate for these data than "traditional" methods (e.g., discriminant function analyses) as pattern recognition employs both linear and non-linear functions and therefore reflects multiple and complex interactions and not just "main-effects." 

More specifically, we used a pattern recognition procedure that has been developed for analysing what are known as *short and wide* data sets [i.e., data sets that contain more variables than cases] as the present data set are. This pattern recognition procedure has been used in several recent studies to examine differences between athletes of different performance levels [e.g., @Gullich2019; @Jones2019a; @Jones2019b; @Jones2020].

This procedure is a three-part process. First, we aimed to identify a set of features which correlated well with the class but had a low correlation with one another (feature selection). Second, we tested the ability of this feature subset to correctly classify the candidates according to the criterion variable for that analysis (classification). Finally, we refined the feature subset to identify the simplest solution that best explained the data (recursive feature elimination). We completed all analyses using WEKA 3-9-3 open source software issued under the GNU General Public License version 3 [@Bouckaert2018; @Frank2016]. WEKA is a machine learning workbench with a collection of algorithms widely used for data mining, machine learning, and pattern recognition.


#### Preprocessing.

Using the same data to train and test a model leads to the risk of over-fitting and classification rates being artificially inflated as all the data have been "seen" during the feature selection stage. This phenomenon is known as "peeking" [@Kuncheva2018; @Reunanen2003a; @Smialowski2010] and can be avoided by holding some data out of the feature selection stage as outlined below. For a given classification problem, the ideal way to perform the analyses would be as follows:

  1. Given $N$ cases, randomly select $x$ cases, where $\frac{N}{3} > x > \frac{N}{10}$, for each class of the criterion variable to be held-out as a test data set $D_{test}$ and the remaining candidates become the training data $D_{train}$.
  2. Prepare both the $D_{train}$ and $D_{test}$ data sets separately (e.g., standardising the data).
  3. Perform the feature selection process using $D_{train}$.
  4. Carry out the classification process using $D_{train}$ using k-fold cross validation to select the best model.
  5. Carry out the classification process on $D_{test}$ using $D_{train}$ to train the classification model chosen in step 4.

We used this procedure when  we had enough available data. When enough data did not exist to create separate $D_{test}$ and $D_{train}$ sets we included all cases in D_train. $D_{train}$.


#### Feature Selection.

Feature selection aims to remove irrelevant and redundant variables from the analysis to improve the predictive performance of models [@Guyon2003]. In this study, we used three techniques designed to improve the performance of feature selection when applied to short and wide data: the use of multiple feature selection algorithms, carrying out feature selection in a *vertically distributed* fashion, and using *leave-one-out cross-validation*.

When using multiple feature selection algorithms, the greater the number of algorithms that select a feature, the more confident one can be that the feature is important as it is less likely that the feature has been chosen by chance [@Visa2011]. In this study, we used four feature selection algorithms: Fast Based Correlation Filter [FCBF; @Yu2003], Correlation Attribute Evaluator [CAE; @Bouckaert2018], Relief-f [@Kira1992], and Support Vector Machine - Recursive Feature Elimination [SVM-RFE; @Guyon2002]. 

CAE, Relief-f, and SVM-RFE rank all features in order of merit (magnitude of relationship), whereas FCBF selects a subset of features that are highly correlated with the class but not with one another. As only FCBF provides a subset of features, we selected the top 20 features from the rankings provided by the other three algorithms (if the attribute merit was greater than zero)^[If there were fewer than 20 features in the subset that feature selection was being applied to, we selected the top 10 features. There were more than 10 features in all subsets.]. All four algorithms are well established feature selection methods and the most important point to note about these four algorithms is that each works in very a different way.

When applied to a data set, using multiple feature selection algorithms yields several feature subsets for each classification problem based on the agreement between the algorithms about the importance of each feature. Features that are not selected or are only selected by one feature selection algorithm are discarded. The following feature subsets are then created from the remaining features: features selected by at least two feature selection algorithms (2s), features selected by at least three feature selection algorithms (3s), and features selected by all four feature selection algorithms (4s). In this study we only retained feature subsets with at least five features.

We ran each algorithm using a leave-one-out cross-validation (LOO-CV) protocol. LOO-CV is a special case of $K$-fold cross-validation, where $K = N$, as it reduces the impact of each object on the feature selection process by increasing the generalisability of the model [@Hastie2009; @DeRooij2020]. Each data set was split into $K$ parts or *folds*, with each fold having an approximately equal number of cases. The $K$th fold is then removed from the data and the feature selection algorithm is then applied to the remaining data, with each feature being assigned a merit score (or being selected/not for CFS), once this has been repeated $K$ times the merit score for each attribute is averaged across the $K$ iterations.

Feature selection was carried out separately in a both a vertically distributed and *centralised* fashion. Centralised feature selection includes all features at once. Whereas, vertically distributed feature selection applies the algorithm to several distinct subsets of features, before merging the features selected in each vertical partition, to form a previously unseen feature subset, and applying the algorithm to the new merged feature subset. There is evidence that this process can improve classification rates as it results in "a more balanced feature/sample ratio" reducing the likelihood of overfitting problems [@Bolon-Canedo2015a, p 137]. A list of the vertical partitions for this study can be found in [Appendix B](#pra-method-details).

In this study, the combination of multiple feature selection algorithms and vertically distributed meant that we created 2s, 3s, and 4s for each feature subset. These were then merged across the vertical partitions based on level of agreement before reapplying the four feature selection algorithms to the new merged feature subsets. For example, all of the 2s across the feature subsets were combined to form a new merged subset and the feature selection process was then reapplied to this new feature subset, potentially resulting in a further 3 feature subsets for each classification problem (i.e., merged 2s, merged, 3s, merged 4s) (see [Appendix B](#pra-method-details) for a worked example). 

The merging process was carried out for all feature subsets as well as for the survey-based feature subsets and DLOG based feature subsets separately. This process resulted in several candidate feature subsets to be carried forward to the classification stage of the analysis. For each classification problem, there were at least 2s and 3s for the following candidate feature subsets: centralised, each vertical partition of the data, merged, merged survey, merged DLOG.


#### Classification.

In order to evaluate the predictive performance of each candidate feature subset, we performed *initial classification* experiments using WEKA's Experiment Environment [@Bouckaert2018; @Frank2016]. As in the feature selection step, classification experiments used four classification algorithms and LOO-CV given the nature of the data. We used the following classification algorithms with their default settings: Naïve Bayes [NB; @John1995], Sequential Minimal Optimization [SMO; @Platt1998], Instance Based Learning [IBk; @Aha1991], J48 Decision Tree [J48; @Quinlan1993]. As with feature selection, the more consistent the results from each algorithm (classification accuracy) for a feature subset, the more confidence we can place in the predictive validity of that subset. This process returned a classification rate for each feature subset and classifier^[It is important to note that as all of the data have been seen during the feature selection stage the classification rates may be slightly higher than they would be for previously unseen data [@Kuncheva2018; @Smialowski2010].].

Having conducted the initial classification experiments, we sought to identify more parsimonious models, potentially with higher classification accuracies using the Recursive Feature Elimination (RFE) method [@Guyon2002]. This process is known as *final classification*. To complete final classification we took each feature subset with more than five features in, examined the normalised SMO weight provided by the SMO classifier and removed the feature with the lowest weight before re-running the four classifiers on the, now smaller, feature subset. This process continued in an iterative fashion until all features with a SMO weight < .4 had been removed, the iteration with the best classification rate was then retained as a new feature subset.


#### Model Selection.

The feature selection process yielded `r nrow(G5_male_GTA18m_train_PRA)` getting to assessment models for both male and female candidates and `r nrow(G5_zsex_ftp_train_PRA)` first time pass models. For each of the classification problems listed above, we selected the "best" models. It is important to recognise that these three solutions are not the only useful ones; however, they were the models that best classified the training data. It is also important to note that we considered the classification profile for each model, rather than just the mean score. It is not uncommon for one classifier (often J48) to perform much worse than the others, therefore if a model performed well with three classifiers and poorly with another, that model was preferred to one that performed better on average (i.e., had a greater mean classification accuracy). For example, consider the classification profiles of the following models: Model A - NB = 85, SMO = 90, IBk = 85, J48 = 50 (mean = 77.5) and Model B - NB = 80, SMO = 80, IBk = 80, J48 = 80 (mean = 80). In this example we would prefer Model A to Model B.


#### Model Testing and Validation.

Where test and validation data were available, we used the training data to train the models to predict the class of each object in these data sets. The performance of these predictions was assessed using percentage classification accuracy as in the model selection stage of the analysis.


## Results {#ml-pra-results}

Below are the results of the analyses for each classification problem. As outlined in the [Participants](#ml-pra-participants) section of the [Method](#ml-pra-method), we were able to create training data sets for all three problems and we were therefore able to identify models for all three problems. However, there were only enough data to create test and validation data sets for male candidates getting to assessment within 18 months of training, therefore we were only able to carry out model testing and validation for this classification problem.

### Male Candidates - Getting to Assessment{#males-gta}

#### Model Selection.

```{r}
g5_male_gta_18m_fs %>% 
  filter(feature_subset %in% 
           c("Merged_survey_2s_2s", "Centralised_3s")) %>% 
  .$variables %>% 
  str_split(string = ., pattern = ", ") %>% 
  unlist() %>% 
  table() %>% 
  as.data.frame() %>% 
  transmute(var = as.character(.),
            Freq = Freq) %>% 
  arrange(var) -> g5_male_gta_18m_features

g5_male_gta_18m_retained <- 
  read_xlsx("chapter_3_data/v1/g5_gta18m_male_rfe_classification.xlsx", sheet = "variables") %>% 
  filter(feature_subset != "Centralised_2s_RFE")

g5_male_gta_18m_retained_vars_unique <- g5_male_gta_18m_retained$variables %>% str_split(pattern = ", ") %>% unlist() %>% unique()

g5_male_gta_18m_retained_vars <- 
  g5_male_gta_18m_retained$variables %>% str_split(pattern = ", ")

g5_male_gta_18m_retained_vars_count <- 
g5_male_gta_18m_retained_vars %>% unlist() %>% table() %>% as.data.frame() %>% arrange(Freq)

g5_male_gta_18m_rfe_classification_rates <- 
  read_xlsx("chapter_3_data/v1/g5_gta18m_male_rfe_classification.xlsx", sheet = "classification_rates")


```

Using the feature selection method outlined above, we created `r nrow(G5_male_GTA18m_train_PRA)` different feature subsets using the training data for classifying male candidates as assessed or not assessed within 18 months of their training course. To evaluate the performance of each feature subset, we carried out initial classification on all `r nrow(G5_male_GTA18m_train_PRA)` feature subsets. There were `r nrow(filter(G5_male_GTA18m_train_PRA, between(median, 80, 90)))` feature subsets that were *very good* at classifying the data. One of these was from an original feature subset, two were from the centralised feature selection, and the remainder were merged feature subsets from the decentralised feature selection. The performance of each feature subset can be seen in [Appendix C](#ml-pra-supp-info) Table \@ref(tab:g5-male-gta18m-train-models).

We retained the best two of these feature subsets: the "Merged survey 2s 2s" and the "Centralised 3s" feature subsets to carry forward to the final classification step of the analysis. There were `r nrow(g5_male_gta_18m_features)` unique features between the two feature subsets, `r (numbers2words(nrow(filter(g5_male_gta_18m_features, Freq == 2))))` features were common to both feature subsets, and `r numbers2words(nrow(filter(g5_male_gta_18m_features, Freq == 1)))` features were contained in only one of the feature subsets.

For the final classification step, we carried out the recursive feature elimination process on the two feature subsets separately, Table \@ref(tab:g5-male-gta-18m-final-classification-rates) shows the results of this process. In the Merged survey 2s 2s feature subset only one feature was removed; IBk and J48 saw improvements in classification rates, SMO decreased in performance, and NB remained the same. The RFE feature subset was retained as it had fewer features and had a better classification profile---both in terms of average and consistency. In the Centralised 3s feature subset, again, only one feature was removed, improving the performance of NB and SMO, but substantially reducing the performance of IBk, and the performance of J48 remained the same. This time, we retained the original feature subset as it had a better classification profile than the RFE feature subset. Given that neither the Centralised 3s or Merged survey 2s 2s RFE feature subsets performed better than the other, we retained both as predictive models for the model testing and validation steps. 


```{r g5-male-gta-18m-final-classification-rates}
G5_male_GTA18m_train_PRA %>% 
  filter(Dataset %in% c("Merged_survey_2s_2s", "Centralised_3s")) %>% 
  bind_rows(g5_male_gta_18m_rfe_classification_rates %>% 
              filter(dataset == "training" & 
                       feature_subset %in% c("Merged_survey_2s_2s_RFE", "Centralised_3s_RFE")) %>% 
              transmute(Dataset = feature_subset,
                        n = n_features,
                        NB = NB,
                        SMO = SMO,
                        IBk = IB6,
                        J48 = J48)) %>% 
  mutate(`Feature subset` = str_replace_all(Dataset, "_", " ")) %>% 
  select(-c(mean, median, Dataset)) %>% 
  select(`Feature subset`, everything()) %>% 
  my_table(caption = "Male candidates getting to assessment within 18 months of training, classification rates for feature subsets included in final classification.") %>% 
  add_header_above(header = c(" " = 2, "Classification rate (%)" = 4)) %>% 
  kableExtra::group_rows(group_label = "Initial classification", start_row = 1, end_row = 2) %>% 
  kableExtra::group_rows(group_label = "Final classification", start_row = 3, end_row = 4) %>% 
  add_footnote("Note: n = number of features, NB = Naïve Bayes, SMO = Sequential Minimal Optimization, IBk = Instance Based Classified, J48 = J48 Decision Tree.", notation = "none") %>% 
  column_spec(1, width = "5cm") %>% 
  column_spec(2:6, width = "1cm")
  
```


#### Model Testing and Validation.

We tested both models selected above on the test data. Across all four classification algorithms, each model classified the test data with 90% accuracy, except for IBk in the Merged survey 2s 2s RFE model, which had a classification rate of 80%. In both models, Case 8 was misclassified by NB, SMO, and IBk and Case 3 was misclassified by J48, Case 7 was also misclassified in the Merged survey 2s 2s RFE model by IBk. The performance of these models on the test data is evidence that the models are not over-fitted to the training data as they are similar to the classification rates from the training data, thus increasing our confidence that the variables selected are important discriminatory variables.

When applied to the validation data (i.e., candidates who completed the survey 12-18 months post-training), the performance of the models was consistent across both the classifiers and models but was lower than in the test data (see Table \@ref(tab:g5-male-gta18m-valid)). It is important to note that this data set included candidates who were assessed but more than 18 months-post training, which neither the training nor test data sets did. Therefore, this reduction in classification accuracy may not be surprising. 


```{r g5-male-gta18m-valid}

caption <- paste0("Group 5 male candidates getting to assessment within 18 months of training, test and validation data model performance.")

g5_male_gta_18m_rfe_classification_rates %>% 
  filter(dataset != "training" & feature_subset %in% 
           c("Merged_survey_2s_2s_RFE", "Centralised_3s")) %>% 
  transmute(`Feature subset` = str_replace_all(feature_subset, "_", " "),
            Dataset = dataset,
            n = n_cases,
            NB = NB,
            SMO = SMO,
            IB6 = IB6,
            J48 = J48) %>% 
  arrange(Dataset) %>% 
  select(-c(Dataset, n)) %>% 
  my_table(caption = caption) %>% 
  add_header_above(header = c(" " = 1, "Classification rate (%)" = 4)) %>% 
  kableExtra::group_rows(group_label = "Test data (n = 10)", start_row = 1, end_row = 2) %>% 
  kableExtra::group_rows(group_label = "Validation data (n = 60)", start_row = 3, end_row = 4) %>% 
  add_footnote("Note: n = number of candidates, NB = Naïve Bayes, SMO = Sequential Minimal Optimization, IBk = Instance Based Classified, J48 = J48 Decision Tree.", notation = "none") %>% 
  column_spec(1, width = "6cm") %>% 
  column_spec(2:5, width = "1cm")


time_to_writing_g5_pred_not_assessed <- 
  g5_male_GTA_18m_pred_perform %>% 
  filter(assessed == FALSE) %>% 
  mutate(time_to_writing = 
              interval(first_training_date, ymd("20200609"))/years(1)) %>% 
  summarise(min = min(time_to_writing),
            max = max(time_to_writing),
            mean = mean(time_to_writing),
            sd = sd(time_to_writing),
            median = median(time_to_writing)) %>% 
  round(2)
```


To better understand the prediction errors in the validation data set, we assigned a *voted predicted class* for each candidate based on the average predicted class across the classifier ensemble^[We added a fifth classifier, the Multilayer Perceptron (MLP; @Bishop2006), for these classification analyses to ensure that there were no ties amongst the predicted classes for a given object.] for both the Merged survey 2s 2s RFE and Centralised 3s models. We then split the candidates into groups based on three factors: when they completed the survey, prospectively (i.e., before an assessment) or retrospectively (i.e., after an assessment);  if they had been assessed within 18 months of their training course; and if they had ever been assessed. We then calculated the percentage accuracy of the voted predicted class within the resultant groups and used this to assess the performance of the model, rather than the classification rates of the individual classifiers. The mean classification rates for each group by model combination are shown in Table \@ref(tab:g5-male-GTA-18m-validation-perform).

This analysis shows that, again, both models perform approximately equally well. Both models were extremely good at classifying candidates who had been assessed within 18 months of their training course and responded to the survey after their assessment. Both models were also very good at classifying candidates who were not assessed within 18 months of their training nor had they been at the time of writing ($M_{interval} = `r time_to_writing_g5_pred_not_assessed$mean`$, $SD_{interval} = `r time_to_writing_g5_pred_not_assessed$sd`$ years; over 80% of male candidates who are ever assessed are assessed within this time period (see Figure \@ref(fig:survival-sex)). These groups are the same as the two groups of candidates who were included in the training and test data sets.

However, the models were less good at classifying candidates who completed the survey prospectively and were subsequently assessed. The models were moderately good at classifying those who complete the survey prospectively and were assessed within 18 months of their training course and were extremely poor at classifying candidates who completed the survey prospectively and were assessed more than 18 months after their training course. It is important to note that candidates who were assessed more than 18 months post-training were excluded from the training and test data as the wording of the questions shown to them was not comparable to those who had been assessed within 18 months, which was important for questions asking the candidate to consider the "six months before assessment." Further, given the broader context of these analyses---trying to understand the factors that differentiate those who complete the Mountain Leader from those who do not---this poor classification accuracy may not be important as many of the candidates that are being misclassified are those who are assessed, but more than 18 months post-training.

When candidates who were assessed more than 18 months post-training were excluded from the analysis, the classification rates for both models were `r round((74*77.03 + 19*57.89 + 55*90.90)/(164-25),2)`%, much closer to the classification rates in the training and test data. These findings provide is further evidence that the models are not over-fitted to the training data and are good at classifying candidates who are assessed within 18 months of their training course.

The results from this data set do allow us to place more confidence in the models selected and many of the classification errors may not be important to Mountain Training as some of the candidates who are being misclassified are still going on to be assessed (most within 24 months of their training).


```{r g5-male-GTA-18m-validation-perform}
g5_male_GTA_18m_pred_perform %>% 
  mutate(Survey = case_when(survey_time == "pros" ~ "Prospective",
                            survey_time == "retro" ~ "Retrospective"),
         Merged_survey_2s_2s_RFE_voted = case_when(merged_survey_2s_2s_RFE < 50 ~ 0,
                                     merged_survey_2s_2s_RFE > 50 ~ 1,
                                     merged_survey_2s_2s_RFE == 50 ~ .5),
         Centralised_3s_voted = case_when(centralised_3s < 50 ~ 0,
                                     centralised_3s > 50 ~ 1,
                                     centralised_3s == 50 ~ .5)) %>% 
  group_by(Survey, assessed_18m, assessed) %>% 
  summarise(n = n(),
            Merged_survey_2s_2s_RFE = mean(Merged_survey_2s_2s_RFE_voted) * 100,
            Centralised_3s = mean(Centralised_3s_voted) * 100) %>%
  kable(caption = "Group 5 male candidates getting to assessment within 18 months of training, sub-group prediction model performance.", 
        digits = 2, booktab = TRUE,
        col.names = 
          c("Survey completion", "Assessed within 18 months", "Assessed",
            "n", "Merged survey 2s 2s RFE", "Centralised 3s")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE) %>% 
  column_spec(column = 1, width = "2cm") %>% 
  column_spec(column = 2, width = "2cm") %>% 
  column_spec(column = 3, width = "1cm") %>% 
  column_spec(column = 4, width = ".5cm") %>% 
  column_spec(column = 5, width = "1cm") %>% 
  column_spec(column = 6, width = "1cm") %>% 
  add_header_above(header = c(" " = 4, "Classification rate (%)" = 2)) %>% 
  collapse_rows() 
```


#### Summary.

Given that no single model performed better than all others, we retained two classification models for male candidates: a nine feature model (the Merged survey 3s 3s RFE feature subset) and a seven feature model, a subset of the nine feature model (the Merged 2s 3s and Centralised 3s RFE feature subsets). Figure \@ref(fig:male-GTA-18m-plot-survey-2s-2s-RFE) shows the normalised group means for the training data of the `r length(g5_male_gta_18m_retained_vars[[1]])` unique features included in the Merged 2s 3s model and Figure \@ref(fig:male-GTA-18m-plot-survey-2s-2s-RFE) shows the normalised group means for the training data of the `r numbers2words(length(g5_male_gta_18m_retained_vars[[2]]))` unique features included in the Centralised 3s RFE model. Table \@ref(tab:male-GTA-18m-combined-summary-stats) shows the unstandardised descriptive statistics for the `r length(g5_male_gta_18m_retained_vars_unique)` unique variables in the two models for each of the two classes separately. Both representations of the data can be considered as *stereotypical profiles* for the two classes. It is important that the features within the models are considered in a holistic manner as it is their combination that can correctly classify candidates, not any single feature.

The analyses for this classification problem identified two predictive models with equal performance; the features included in one of which were a subset of the features included in the other. The models were excellent at classifying both the training and test data, which suggests that the model developed using the training data can be generalised to previously unseen data.

The models suggest that, male candidates who are assessed within 18 months of their training course are more likely than male candidates who are not assessed within 18 months of their training course to: feel that they have enough time to become a Mountain Leader, feel more resilient, feel that becoming a Mountain Leader is more important, feel that becoming a Mountain Leader is more important than achieving other life goals, feel that they have made progress towards becoming a Mountain Leader, feel that they have made more progress towards becoming a Mountain Leader than achieving other life goals, feel that they have more resources and skills available to them to become a Mountain Leader than to achieve other life goals, have felt that they had a better understanding of the Mountain Leader qualification before they attended their training course, have felt that it would take a shorter period of time to get from training to assessment, both at the start of and end of their training course, have a greater intention to be assessed by the end of their training course, be trained in the summer, experience less social change post-training, feel that they have less esteem support available, feel that they had done more to prepare effectively for an assessment in the last six-months, feel closer to their ideal level of self-efficacy to look after themselves and others in steep ground/crossing a river, and have QMDs logged in a greater number of mountainous regions 18 months post-training.


```{r male-GTA-18m-plot-survey-2s-2s-RFE, fig.cap="Merged survey 2s 2s RFE: Normalised training group means for male candidates getting to assessment within 18 months of their training course."}

df <- 
  G5_male_GTA_18m_train %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_male_gta_18m_retained_vars[[1]]), Assessed_18months) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(Assessed_18months) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "Assessed_18months", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "Assessed_18months") %>% 
  left_join(select(var_descriptions, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
    fill = 'toself', width = 800, height = 500) %>%
  add_trace(r = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$value,
          subset(df, Assessed_18months == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Not assessed') %>%
  add_trace(r = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$value, 
          subset(df, Assessed_18months == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Assessed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))),
         legend = list(orientation = "h"),
         margin = list(r = 75),
         font = list(family = font, size = 11)) -> p
  
p %>%
  orca(file = "plots/male-GTA-18m-plot-survey-2s-2s-RFE.pdf", width = "30cm", parallel_limit = 2)

if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/male-GTA-18m-plot-survey-2s-2s-RFE.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


```{r male-GTA-18m-plot-centralised-3s, fig.cap="Centralised 3s: Normalised training group means for male candidates getting to assessment within 18 months of their training course."}
df <- 
  G5_male_GTA_18m_train %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_male_gta_18m_retained_vars[[2]]), Assessed_18months) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(Assessed_18months) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "Assessed_18months", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "Assessed_18months") %>% 
  left_join(select(var_descriptions, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
  fill = 'toself', width = 800, height = 400
    ) %>%
  add_trace(r = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$value,
          subset(df, Assessed_18months == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Not assessed') %>%
  add_trace(r = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$value, 
          subset(df, Assessed_18months == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Assessed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))), 
         legend = list(orientation = "h"),
         font = list(family = font, size = 11)) -> p
  
p %>%
  orca(file = "plots/male-GTA-18m-plot-centralised-3s.pdf", width = "30cm", parallel_limit = 2)

if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/male-GTA-18m-plot-centralised-3s.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


```{r male-GTA-18m-combined-summary-stats}
male_GTA_18m_combined_summary_stats <- 
  G5_male_GTA_18m_train %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_male_gta_18m_retained_vars_unique), Assessed_18months) %>% 
  as.data.frame() %>% 
  psych::describeBy(x = ., group = "Assessed_18months")

full_join(male_GTA_18m_combined_summary_stats[[1]] %>% rownames_to_column(), 
          male_GTA_18m_combined_summary_stats[[2]] %>% rownames_to_column(), 
          by = "rowname", suffix = c("_0", "_1")) %>% 
  filter(rowname != "Assessed_18months") %>% 
  left_join(., select(var_descriptions, 
                      c(var_label, var_description, response_options)), 
            by = c("rowname" = "var_label")) %>% 
  mutate(Variable = 
           case_when(
             rowname %in% g5_male_gta_18m_retained_vars[[1]] &
               rowname %in% g5_male_gta_18m_retained_vars[[2]] ~ 
               paste0(var_description, 
                      if (is_latex_output()) {" \\textsuperscript{*†}"} else {
                        "*^†^"}),
             !rowname %in% g5_male_gta_18m_retained_vars[[1]] &
               rowname %in% g5_male_gta_18m_retained_vars[[2]] ~ 
               paste0(var_description, 
                      if (is_latex_output()) {"\\textsuperscript{†}"} else {"^†^"}),
             rowname %in% g5_male_gta_18m_retained_vars[[1]] &
               !rowname %in% g5_male_gta_18m_retained_vars[[2]] ~
               paste0(var_description, "*"))) %>% 
  select(c(Variable, starts_with("mean"), starts_with("median"), starts_with("sd"), starts_with("min"), starts_with("max"))) %>% 
  select(c(Variable, ends_with("_0"), ends_with("_1"))) %>% 
  kable(caption = "Unstandardised group descriptive statistics of the features that discriminate male candidates who are assessed within 18 months of their training course from those who are not.", 
        escape = FALSE,
        digits = 2, booktab = TRUE, 
        col.names = c("Variable", rep(c("mean", "median", "sd", "min", "max"), 2))) %>%
  landscape() %>% 
  column_spec(1, width = "10cm") %>% 
  add_header_above(header = c(" " = 1, "Not assessed" = 5, "Assessed" = 5)) %>% 
  add_footnote(c("Included in the merged survey 2s 2s RFE model", 
                 "Included in the centralised 3s model"), 
               notation = "symbol") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                latex_options = c("hold_position"),
                full_width = FALSE)
```


### Female Candidates - Getting to Assessment

```{r}
g5_female_gta_18m_fs %>%
  filter(feature_subset %in%
           c("Merged_survey_3s_2s", "Merged_3s_3s")) %>%
  .$variables %>% 
  str_split(string = ., pattern = ", ") %>% 
  unlist() %>% 
  table() %>%
  as.data.frame() %>%
  transmute(var = as.character(.),
            Freq = Freq) %>%
  arrange(var) -> g5_female_gta_18m_features

g5_female_gta_18m_retained_models <- 
  read_xlsx("chapter_3_data/v1/g5_gta18m_female_rfe_classification.xlsx", sheet = "variables")

g5_female_gta_18m_retained_vars_unique <- g5_female_gta_18m_retained_models$variables %>% str_split(pattern = ", ") %>% unlist() %>% unique()

g5_female_gta_18m_retained_vars <- 
  g5_female_gta_18m_retained_models$variables %>% str_split(pattern = ", ")

g5_female_gta_18m_retained_vars_count <- 
g5_female_gta_18m_retained_vars %>% unlist() %>% table() %>% as.data.frame() %>% arrange(Freq)

g5_female_gta_18m_rfe_classification_rates <- 
  read_xlsx("chapter_3_data/v1/g5_gta18m_female_rfe_classification.xlsx", sheet = "classification_rates")

```

Using the same feature selection method as for male candidates, we created `r nrow(G5_female_GTA18m_PRA)` feature subsets using the training data, which we then carried out the initial classification procedure on. As in the male candidates’ data, there was no single standout feature subset In this instance two feature subsets (Merged survey 3s 2s and Merged 3s 3s) classified the training data approximately equally well. These two feature subsets were "very good" at classifying the training data and contained `r nrow(g5_female_gta_18m_features)` unique features, `r  nrow(filter(g5_female_gta_18m_features, Freq == 2))` features were included in two of the feature subsets. Table \@ref(tab:g5-female-gta18m-train-models) shows the classification rates for all feature subsets included in the initial classification step.

Both feature subsets were carried forward to the final classification step of the analysis. Seven features were removed from the Merged survey 3s 3s feature subset, improving the classification rate of all four classifiers and three features were removed from the Merged 3s 3s feature subset, however, this did not improve the performance of any classifier, rather, it reduced the performance of two of them (see Table \@ref(tab:g5-female-gta-18m-final-classification-rates)). The Merged survey 3s 2s RFE model (Figure \@ref(fig:female-GTA-18m-plot-combined-merged-survey-3s-2s)) and the Merged 3s 3s (Figure \@ref(fig:female-GTA-18m-plot-combined-merged-3s-3s)) feature subsets were retained as predictive models. Table \@ref(tab:female-GTA-18m-combined-summary-stats) shows the unstandardised descriptive statistics for the `r length(g5_female_gta_18m_retained_vars_unique)` unique variables in the two models, for each of the two classes separately.


```{r g5-female-gta-18m-final-classification-rates}
G5_female_GTA18m_PRA %>% 
  filter(Dataset %in% c("Merged_survey_3s_2s", "Merged_3s_3s")) %>% 
  bind_rows(g5_female_gta_18m_rfe_classification_rates %>% 
              filter(dataset == "combined") %>% 
              transmute(Dataset = feature_subset,
                        n = n_features,
                        NB = NB,
                        SMO = SMO,
                        IBk = IB6,
                        J48 = J48)) %>% 
  mutate(`Feature subset` = str_replace_all(Dataset, "_", " ")) %>% 
  select(-c(mean, median, Dataset)) %>% 
  select(`Feature subset`, everything()) %>% 
  my_table(caption = "Female candidates getting to assessment within 18 months of training, classification rates for feature subsets included in final classification.") %>% 
  add_header_above(header = c(" " = 2, "Classification rate (%)" = 4)) %>% 
  kableExtra::group_rows(group_label = "Initial classification", start_row = 1, end_row = 2) %>% 
  kableExtra::group_rows(group_label = "Final classification", start_row = 3, end_row = 4) %>% 
  add_footnote("Note: n = number of features, NB = Naïve Bayes, SMO = Sequential Minimal Optimization, IBk = Instance Based Classified, J48 = J48 Decision Tree.", notation = "none")
```


The two models retained include `r length(g5_female_gta_18m_retained_vars_unique)` features; `r numbers2words(nrow(filter(g5_female_gta_18m_retained_vars_count, Freq == 2)))` features are included in both models, and `r nrow(filter(g5_female_gta_18m_retained_vars_count, Freq == 1))` features are unique to a single model (nine of which were DLOG variables, therefore could only be included in one model). This level of overlap between the models allows us to place more confidence in the importance of the features included in them.

The features contained in the two models retained suggest that female candidates who are assessed within 18 months of their training course are more likely than female candidates who are not assessed within 18 months of their training course to: feel that they have enough time to become a Mountain Leader; feel that becoming a Mountain Leader is more important; feel that becoming a Mountain Leader is more important than achieving other life goals; feel that they have made progress towards becoming a Mountain Leader; feel that they have made more progress towards becoming a Mountain Leader than achieving other life goals; feel that their training course staff helped them to set goals; feel more confident at the end of training that they could wild camp for two nights in any weather; experience professional change post-training; have an extrinsic second participatory motive for registering; feel that they have done more to prepare for an assessment in the last six-months; have practiced their navigation skills; feel more confident pre-assessment in their skills to: look after themselves and others in steep ground, provide immediate medical care in the mountains, respond appropriately to an emergency, navigate to a chosen point on a map in any conditions, night or day; feel that both their ideal and ought selves would have logged a greater number of QMDs pre-assessment; 18 months post-training: have a greater number of QMD entries, QMDs^[QMD entries can span more than one day.], have visited more mountainous regions to log their QMDs, more QMDs in poor visibility, summited more mountains, and summited more mountains at least 600m high.


```{r female-GTA-18m-plot-combined-merged-survey-3s-2s, fig.cap="Merged survey 3s 2s: Normalised training group means for female candidates getting to assessment within 18 months of their training course."}
df <- G5_female_GTA_18m_comb %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_female_gta_18m_retained_vars[[1]]), Assessed_18months) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(Assessed_18months) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "Assessed_18months", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "Assessed_18months") %>% 
  left_join(select(eshot_vars, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
  fill = 'toself',
  width = 800, height = 500) %>%
  add_trace(r = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$value,
          subset(df, Assessed_18months == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Not assessed') %>%
  add_trace(r = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$value, 
          subset(df, Assessed_18months == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Assessed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))),
         legend = list(orientation = "h"),
         font = list(family = font, size = 11)) -> p
 
p %>%
  orca(file = "plots/female-GTA-18m-plot-combined-merged-survey-3s-2s.pdf", width = "30cm", parallel_limit = 2)

if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/female-GTA-18m-plot-combined-merged-survey-3s-2s.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```

```{r female-GTA-18m-plot-combined-merged-3s-3s, fig.cap="Merged 3s 3s: Normalised training group means for female candidates getting to assessment within 18 months of their training course."}

df <- 
  G5_female_GTA_18m_comb %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_female_gta_18m_retained_vars[[2]]), Assessed_18months) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(Assessed_18months) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "Assessed_18months", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "Assessed_18months") %>% 
  left_join(select(var_descriptions, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
  fill = 'toself',
  width = 800, height = 500) %>%
  add_trace(r = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$value,
          subset(df, Assessed_18months == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 0, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Not assessed') %>%
  add_trace(r = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$value, 
          subset(df, Assessed_18months == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable, 
              subset(df, Assessed_18months == 1, select = c("variable", "value"))$variable[1]),
    name = 'Assessed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))),
         legend = list(orientation = "h"),
         font = list(family = font, size = 11)) -> p
 
p %>%
  orca(file = "plots/female-GTA-18m-plot-combined-merged-3s-3s.pdf", width = "30cm", parallel_limit = 2)

if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/female-GTA-18m-plot-combined-merged-3s-3s.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


```{r female-GTA-18m-combined-summary-stats}
g5_female_gta_18m_retained_vars_unique  <- c("PerceivedAvailableTime", "PPMountainLeaderProgress", "RelativeMLProgress", "RelativeMLImportance", "ParticipatoryMotive2_code", "TrainEfficacy1", "TrainMCBSGoalSetting", "ProfessionalChange", "IdealNumQMDs", "OughtNumQMDs", "Preparation_nav", "PerceivedPreparation", "PreAssessmentSelfEfficacy4", "PreAssessmentSelfEfficacy6", "PreAssessmentSelfEfficacy7", "t18_num_logs_7", "t18_num_days_7", "t18_num_areas_7", "t18_mtn_reg_num_7", "t18_num_poor_vis_7", "t18_num_mtns", "t18_num_600m_mtns")


female_GTA_18m_combined_summary_stats <- 
  G5_female_GTA_18m_train %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_female_gta_18m_retained_vars_unique), Assessed_18months) %>% 
  as.data.frame() %>% 
  select(-c(starts_with("Ideal"), starts_with("Ought"), 
            starts_with("t", ignore.case = FALSE)), starts_with("Ideal"), 
         starts_with("Ought"), starts_with("t", ignore.case = FALSE)) %>% 
  psych::describeBy(x = ., group = "Assessed_18months")


full_join(female_GTA_18m_combined_summary_stats[[1]] %>% rownames_to_column(), 
          female_GTA_18m_combined_summary_stats[[2]] %>% rownames_to_column(), 
          by = "rowname", suffix = c("_0", "_1")) %>% 
  filter(rowname != "Assessed_18months") %>% 
  left_join(., select(var_descriptions, c(var_label, var_description)), 
            by = c("rowname" = "var_label")) %>% 
  mutate(Variable = 
           case_when(rowname %in% g5_female_gta_18m_retained_vars[[1]] &
               rowname %in% g5_female_gta_18m_retained_vars[[2]] ~ 
               paste0(var_description, 
                      if (is_latex_output()) {" \\textsuperscript{*†}"} else {"*^†^"}),
             !rowname %in% g5_female_gta_18m_retained_vars[[1]] &
               rowname %in% g5_female_gta_18m_retained_vars[[2]] ~ 
               paste0(var_description, 
                      if (is_latex_output()) {"\\textsuperscript{†}"} else {"^†^"}),
             rowname %in% g5_female_gta_18m_retained_vars[[1]] &
               !rowname %in% g5_female_gta_18m_retained_vars[[2]] ~
               paste0(var_description, "*")),
         Variable = str_remove_all(string = Variable, 
                                   pattern = "Number of "),
         Variable = str_remove_all(string = Variable,
                                   pattern = "Pre-assessment efficacy to ")) %>% 
  select(c(Variable, starts_with("me"), starts_with("sd"))) %>% 
  select(c(Variable, ends_with("_0"), ends_with("_1"))) %>%
  kable(caption = "Group 5 female candidates getting to assessment within 18 months of training, unstandardised group descriptive statistics", escape = FALSE,
        digits = 2, booktab = TRUE, 
        col.names = c("Variable", rep(c("mean", "median", "sd"), 2))) %>%
  landscape() %>% 
  column_spec(1, width = "12cm") %>% 
  add_header_above(header = c(" " = 1, "Not assessed" = 3, "Assessed" = 3)) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                #latex_options = "striped",
                full_width = FALSE) %>% 
  group_rows(group_label = "Pre-assessment efficacy to...", 
             start_row = 11, end_row = 13) %>% 
  group_rows(group_label = "Number of...", start_row = 14, end_row = 22) %>% 
  add_footnote(label = c("Included in Merged survey 3s 2s RFE", 
                         "Included in Merged 3s 3s"),
               notation = "symbol")

```


### Passing First Time


```{r}
g5_zsex_ftp_fs %>% 
  filter(feature_subset %in% 
           c("Merged_survey_2s_3s", "Centralised_2s")) %>% 
  .$variables %>%
  paste(collapse = ",") %>%
  str_extract_all("[0-9]+") %>% 
  table() %>% 
  as.data.frame() %>% 
  transmute(var = as.numeric(as.character(.)),
            Freq = Freq) %>% 
  arrange(var) -> g5_zsex_ftp_features


g5_ftp_rfe_classification_vars_unique <- read_xlsx("chapter_3_data/v3/g5_ftp_rfe_classification.xlsx", sheet = "variables")$variables %>% str_split(pattern = ", ") %>% unlist() %>% unique()

g5_ftp_rfe_classification_vars <- 
  read_xlsx("chapter_3_data/v3/g5_ftp_rfe_classification.xlsx", sheet = "variables")$variables %>% str_split(pattern = ", ")

g5_ftp_rfe_classification_vars_count <- 
g5_ftp_rfe_classification_vars %>% unlist() %>% table() %>% as.data.frame() %>% arrange(Freq)

g5_ftp_rfe_classification_rates <- 
  read_xlsx("chapter_3_data/v3/g5_ftp_rfe_classification.xlsx", sheet = "classification_rates")
```


The feature selection process yielded `r nrow(G5_zsex_ftp_train_PRA)` feature subsets that were all carried forward to the initial classification step. Again, there was no single feature subset that out-performed the others. The two best feature subsets (Merged survey 2s 3s and Centralised 2s) were "good" at classifying the data. These two feature subsets contained `r nrow(g5_zsex_ftp_features)` unique variables, `r  numbers2words(nrow(filter(g5_zsex_ftp_features, Freq == 2)))` of these features are included in both feature subsets. Table \@ref(tab:g5-zsex-ftp-train-models) shows the classification rates for all feature subsets included in the initial classification step.

These two feature subsets were carried forward to the final classification step. Two features were removed from the Merged survey 2s 3s feature subset and six features were removed from the Centralised 2s model. In both instances, this process resulted in the classification rates for two classifiers improving and one decreasing; these changes made the classification profile more consistent (see Table \@ref(tab:g5-zsex-ftp-final-classification-rates)). On this basis the Merged survey 2s 3s RFE model (Figure \@ref(fig:zsex-ftp-merged-survey-2s-3s-rfe-plot)) and the Centralised 2s (Figure \@ref(fig:zsex-ftp-centralised-2s-RFE-plot)) were retained as predictive models. Table \@ref(tab:ftp-combined-summary-stats) shows the unstandardised descriptive statistics for each feature, retained in the two models, within the two classes.


```{r g5-zsex-ftp-final-classification-rates}
G5_zsex_ftp_train_PRA %>% 
  filter(Dataset %in% c("Merged_survey_2s_3s", "Centralised_2s")) %>% 
  bind_rows(g5_ftp_rfe_classification_rates %>% 
              filter(dataset == "combined") %>% 
              transmute(Dataset = feature_subset,
                        n = n_features,
                        NB = NB,
                        SMO = SMO,
                        IBk = IB6,
                        J48 = J48)) %>% 
  mutate(`Feature subset` = str_replace_all(Dataset, "_", " ")) %>% 
  select(-c(mean, median, Dataset)) %>% 
  select(`Feature subset`, everything()) %>% 
  my_table(caption = "Female candidates getting to assessment within 18 months of training, classification rates for feature subsets included in final classification.") %>% 
  add_header_above(header = c(" " = 2, "Classification rate (%)" = 4)) %>% 
  kableExtra::group_rows(group_label = "Initial classification", start_row = 1, end_row = 2) %>% 
  kableExtra::group_rows(group_label = "Final classification", start_row = 3, end_row = 4) %>% 
  add_footnote("Note: n = number of features, NB = Naïve Bayes, SMO = Sequential Minimal Optimization, IBk = Instance Based Classified, J48 = J48 Decision Tree.", notation = "none")
```


The two models retained include `r length(g5_ftp_rfe_classification_vars_unique)` features; `r numbers2words(nrow(filter(g5_ftp_rfe_classification_vars_count, Freq == 2)))` features are included in both models, and `r numbers2words(nrow(filter(g5_ftp_rfe_classification_vars_count, Freq == 1)))` features are unique to a single model (eight of which were DLOG features, therefore could only be included in one of the models). The features in the two classification models suggest that candidates who pass their first assessment are more likely than those who do not to: live closer to the mountains, feel that the training course staff were involved in their development, experience less inter-personal conflict on their training course, feel that their course staff used observation and effective questioning skills more often, feel that they have more informational support available to them, log less experience below the standard for the Mountain Leader (Hill/Moorland walking), and log more experience above the standard of the Mountain Leader (Alpine Climbing). 


```{r zsex-ftp-merged-survey-2s-3s-rfe-plot, fig.cap="Merged survey 2s 3s: Normalised training group means for candidates passing their first assessment."}
df <- g5_zsex_ftp_consol %>% 
  select(-c(CandidateId)) %>% 
  select(all_of(g5_ftp_rfe_classification_vars[[1]]), FirstTimePass) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(FirstTimePass) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "FirstTimePass", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "FirstTimePass") %>% 
  left_join(select(var_descriptions, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
  fill = 'toself',
  width = 500, height = 300) %>%
  add_trace(r = c(subset(df, FirstTimePass == 0, select = c("variable", "value"))$value,
          subset(df, FirstTimePass == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, FirstTimePass == 0, select = c("variable", "value"))$variable, 
              subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable[1]),
    name = 'Did not pass') %>%
  add_trace(r = c(subset(df, FirstTimePass == 1, select = c("variable", "value"))$value, 
          subset(df, FirstTimePass == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable, 
              subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable[1]),
    name = 'Passed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))),
         legend = list(orientation = "h"),
         font = list(family = font, size = 11)) -> p

  p %>%
    orca(file = "plots/zsex-ftp-merged-survey-2s-3s-rfe-plot.pdf", width = "15cm", parallel_limit = 2)
  
if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/zsex-ftp-merged-survey-2s-3s-rfe-plot.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


```{r zsex-ftp-centralised-2s-RFE-plot, fig.cap="Centralised 2s RFE: Normalised training group means for candidates passing their first assessment - survey variables."}
df <- 
  g5_zsex_ftp_consol %>% 
  select(-c(CandidateId)) %>% 
  select(all_of(g5_ftp_rfe_classification_vars[[2]]), FirstTimePass) %>% 
  lapply(normalize) %>% 
  as.data.frame() %>% 
  group_by(FirstTimePass) %>% 
  summarise_all(mean) %>% 
  reshape2::melt(id.vars = "FirstTimePass", 
                 measure.vars = colnames(.)) %>% 
  filter(variable != "FirstTimePass") %>% 
  left_join(select(var_descriptions, c(var_label, var_description_short)),
            by = c("variable" = "var_label")) %>% 
  mutate(variable = as.character(coalesce(var_description_short, variable)))

plot_ly(type = 'scatterpolargl',
  mode = "lines+markers",
  fill = 'toself',
  width = 800, height = 450) %>%
  add_trace(r = c(subset(df, FirstTimePass == 0, select = c("variable", "value"))$value,
          subset(df, FirstTimePass == 0, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, FirstTimePass == 0, select = c("variable", "value"))$variable, 
              subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable[1]),
    name = 'Did not pass') %>%
  add_trace(r = c(subset(df, FirstTimePass == 1, select = c("variable", "value"))$value, 
          subset(df, FirstTimePass == 1, select = c("variable", "value"))$value[1]),
    theta = c(subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable, 
              subset(df, FirstTimePass == 1, select = c("variable", "value"))$variable[1]),
    name = 'Passed') %>%
  layout(polar = list(radialaxis = list(visible = T, range = c(0,1))),
         legend = list(orientation = "h"),
         margin = list(t = 50),
         font = list(family = font, size = 11)) -> p

  p %>%
    orca(file = "plots/zsex-ftp-centralised-2s-RFE-plot.pdf", width = "15cm", parallel_limit = 2)
  
if (knitr::is_html_output()) {
  p
} else
{
  include_graphics("plots/zsex-ftp-centralised-2s-RFE-plot.pdf",
                   auto_pdf = TRUE, dpi = 200)
}

```


```{r ftp-combined-summary-stats}
ftp_consol_summary_stats <- 
  g5_zsex_ftp_consol %>% 
  select(-c(CandidateId, SexId)) %>% 
  select(all_of(g5_ftp_rfe_classification_vars_unique), FirstTimePass) %>% 
  as.data.frame() %>% 
  psych::describeBy(x = ., group = "FirstTimePass")

full_join(ftp_consol_summary_stats[[1]] %>% rownames_to_column(),
          ftp_consol_summary_stats[[2]] %>% rownames_to_column(),
          by = "rowname", suffix = c("_0", "_1")) %>% 
  filter(rowname != "FirstTimePass") %>% 
  left_join(., select(var_descriptions, c(var_label, var_description)),
            by = c("rowname" = "var_label")) %>% 
  mutate(Variable = 
           case_when(rowname %in% g5_ftp_rfe_classification_vars[[1]] &
                       rowname %in% g5_ftp_rfe_classification_vars[[2]] ~
                       paste0(var_description, 
                              if (is_latex_output()) {"\\textsuperscript{*†}"} 
                              else {"*^†^"}),
                     !rowname %in% g5_ftp_rfe_classification_vars[[1]] &
                       rowname %in% g5_ftp_rfe_classification_vars[[2]] ~ 
                       paste0(var_description, 
                              if (is_latex_output()) {"\\textsuperscript{†}"} 
                              else {"^†^"}),
                     rowname %in% g5_ftp_rfe_classification_vars[[1]] &
                       !rowname %in% g5_ftp_rfe_classification_vars[[2]] ~
                       paste0(var_description, "*")),
         Variable = str_remove_all(string = Variable,
                                   pattern = "Number of ")) %>% 
  select(c(Variable, starts_with("me"), starts_with("sd"))) %>% 
  select(c(Variable, ends_with("_0"), ends_with("_1"))) %>% 
  kable(caption = "Group 5 candidates passing first time, unstandardised group descriptive statistics", 
        digits = 2, booktab = TRUE, escape = FALSE,
        col.names = c("Variable", rep(c("mean", "median", "sd"), 2))) %>%
  column_spec(1, width = "12cm") %>% 
  landscape() %>% 
  add_header_above(header = c(" " = 1, "Not passed" = 3, "Passed" = 3)) %>% 
  kableExtra::group_rows(group_label = "Number of...", start_row = 8, end_row = 14) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE) %>% 
  add_footnote(label = c("Included in Merged survey 2s 3s RFE",
                         "Included in Centralised 2s RFE"),
               notation = "symbol")
```


## Discussion {#ml-pra-discussion}

In this study, we aimed to identify important factors that discriminated candidates who (a) having been trained, went on to be assessed within 18 months of training from those who did not (for female and male candidates separately); and (b) having got to their first assessment, pass first time from those who do not. To achieve these aims we considered a wide range of potentially relevant variables (informed by the results of a large qualitative study) and used pattern recognition analyses to analyse data collected from candidates. The use of the pattern recognition analyses allowed us account for the multifaceted nature of becoming a Mountain Leader by considering variables from several domains: personality, socio-demographics, intentions and expectations, motivation, experience of training, experiences post-training, social support, self-efficacy, and experience of assessment if relevant.

We were able to identify predictive models for all three classification problems. The results presented above suggest that there is no one single factor that is important for discriminating candidates, but there are combinations of factors that are important. Whilst there is some overlap between the factors selected in each classification problem, there are factors unique to each problem. As such, we will discuss the results of each problem separately before discussing the commonalities.

Below we discuss the results of each analysis using relevant theory to help explain some of the relationships that may exist between the variables in the final classification models. It is important to reiterate that the variables included in these models are the best discriminatory variables, therefore there may be other variables that are not included in the models but are very important for candidates getting to an assessment/passing their first assessment. For example, none of the variables included as important discriminatory variables are included in either of the first time pass models, but it is likely that in order to pass their first assessment, candidates must possess both the characteristics identified as important in the first time pass analyses *and* the characteristics identified as important for getting to assessment.


### Male Candidates - Getting to Assessment

We identified two predictive models that were both very good at correctly classifying the training data (up to 90.91% accuracy) and test data (up to 90% accuracy) for male candidates getting to assessment within 18 months of training. These models were also able to predict if male candidates who completed the survey more than 12 months, but less than 18 months post-training would be assessed within 18 months of their training with good accuracy (up to 76.67%).


#### Model Performance.

The results from the test data set demonstrate that the three-step feature selection process applied to the training data resulted in a model that was not over-fitted to the data and can therefore be generalised beyond the training data. Interestingly, the reduction in classification rate one may expect due to the peeking effect [@Kuncheva2018; @Reunanen2003a] was not observed in these data. Whilst we recognise that the absence of evidence is not evidence of absence, this is one of few studies that we are aware of which have used this three step process and then tested the predictive models identified on previously unseen data [see @Jones2019b; @Jones2020] and none of these studies have seen a reduction in the classification rates when models are applied to unseen test data. This observation allows us to place more confidence in the three-step method when applied to similar classification problems (i.e., classification problems using multifaceted, short and wide data sets).

There are three main reasons the classification rates for the models when applied to the validation data may be lower than in the training and test data. Firstly, in the training and test data candidates who had been assessed more than 18 months post-training were excluded from the analyses. Secondly, the validation data were collected from candidates who had completed their training less than 18 months prior to training; these candidates were not at the same point in the pathway when they answered the survey, as those included in the training and test data were. Therefore, their answers may have been different to those they would have given six-months later, which would be particularly important for variables that asked candidates about the last six-months. For example, a candidate may not have felt that they had made much progress towards becoming a Mountain Leader 6-12 months post-training and then may make progress in the 12-18 month period. Finally, it is possible that candidates are creating narratives in their minds based on whether they have been assessed and that had they answered the survey before being assessed, then their answers may have been different.

The performance of the two models on the training, test, and validation data sets allows us to place confidence in the importance of the features included in the models for discriminating male candidates who are assessed within 18 months of their training course from those who are not.


#### Features Selected.

Male candidates who were assessed within 18 months of their training course could be discriminated from male candidates who had not been by a combination of features. The *theory of planned behaviour* [@Ajzen1991; @Ajzen1986] provides a useful framework for discussing many of these variables. The attitudes of a candidate towards being assessed will likely be informed by the importance of becoming a Mountain Leader, their understanding of the qualification, and the time they expect it will take them to get from their training course to an assessment. Put another way, how a candidate feels that becoming a Mountain Leader will fit into their life may influence their attitude towards being assessed. In addition, the amount of time that candidates feel they have available to become a Mountain Leader in and their perceived efficacy to become a Mountain Leader will likely influence their perceived behavioural control. According to the theory of planned behaviour the attitudes and perceived behaviour control formed by these variables would form a candidate’s intention to be assessed.

Whilst the observation that candidates who expect it to take them longer to get from training to assessment were less likely to be assessed within a given period may seem elementary, it is important to note that, of the `r G5_male_GTA_18m_train %>% filter(Assessed_18months == 0) %>% .$TrainEndExpTimeToAssess %>% length()` candidates not assessed within 18 months of their training in the training data, only `r G5_male_GTA_18m_train %>% filter(Assessed_18months == 0 & TrainEndExpTimeToAssess > 18) %>% .$TrainEndExpTimeToAssess %>% length()` expected it to take them more than 18 months. One reason for candidates expecting it to take them longer to get from training to assessment may be that they feel they have less available time to become a Mountain Leader than those who expect it to take less time. We were unable to test the direction of this hypothesis with the data from the present study, but it would seem more likely that the less time a candidates feels they have available to become a Mountain Leader, the longer they would expect it to take them to get to an assessment, rather than thinking that they do not have time available to become a Mountain Leader because they expect it to take them longer to get to assessment.

There are some aspects of the personal goal literature that can be used to help understand the current results. Firstly, there is a substantial body of evidence that suggests that goals that are proximal in time are more likely to be adhered to [see @Hardy1996; @Weinberg2014]. Secondly, goals that are too difficult or unrealistic may not be accepted [@Kyllo1995]. It has also been suggested that goals that are too difficult may be perceived as threatening, therefore strong self-efficacy to meet goals is important for sustained motivation [@Bueno2008]. Results from an investigation of the interactive effects of goal importance and self-efficacy on progress towards multiple goals found that goal importance moderated the positive effects of self-efficacy on goal progress, such that goal progress was lower when goal importance was low and self-efficacy was high than when both goal-importance and self-efficacy were high [@Beattie2015].

Further, experiencing social change after a training course may mean that candidates have more or less available time and/or change their  importance in becoming a Mountain Leader. The question used in the survey did not ask if candidates had more or less resources (e.g., available time) because of this change, however given that more social change a candidate experienced, the less likely they were to be assessed within 18 months, it would be reasonable to assume that these social changes are more likely to leave candidates with less, rather than more, resources to become Mountain Leaders. If candidates who expect to take longer do take longer, then there will be more opportunities for barriers to prevent them pursuing and/or attaining that goal .

```{r}
r1 <- ltm::biserial.cor(G5_male_GTA_18m_train$TrainStartAssessIntention, G5_male_GTA_18m_train$Assessed_18months, level = 2) %>% round(2)
r2 <- ltm::biserial.cor(G5_male_GTA_18m_train$TrainEndAssessIntention, G5_male_GTA_18m_train$Assessed_18months, level = 2) %>% round(2)
r3 <- ltm::biserial.cor(G5_male_GTA_18m_test$TrainStartAssessIntention, G5_male_GTA_18m_test$Assessed_18months, level = 2) %>% round(2)
r4 <- ltm::biserial.cor(G5_male_GTA_18m_test$TrainEndAssessIntention, G5_male_GTA_18m_test$Assessed_18months, level = 2) %>% round(2)
r5 <- ltm::biserial.cor(G5_male_GTA_18m_pred$TrainStartAssessIntention, G5_male_GTA_18m_pred$Assessed_18months, level = 2) %>% round(2)
r6 <- ltm::biserial.cor(G5_male_GTA_18m_pred$TrainEndAssessIntention, G5_male_GTA_18m_pred$Assessed_18months, level = 2) %>% round(2)
```


Candidates reported their intention to be assessed at various stages of the pathway (at registration, at the start of their training course, and at the end of their training course), but only their intention to be assessed at the end of their training course was selected as an important discriminatory variable. Not all candidates have a good understanding of the Mountain Leader qualification and therefore may be attending a training course to find out more about the qualification [see Qualitative chapter](#ml-qualitative). Above, we suggested that this understanding may influence candidates' attitudes towards being assessed, but it is also likely to influence their intention to be assessed. Therefore, we would suggest that the strength of a candidate's intention to be assessed at the end of the training course is more important than their intention at the start of the training course because it is based on a more accurate and complete  understanding of the qualification^[The correlations between intention to be assessed at the start of the training course and at the end of the training course with being assessed 18 month post-training support this suggestion (training data: $r_{start} =$ `r r1`, $r_{end} =$ `r r2`; test data: $r_{start} =$ `r r3`, $r_{end} =$ `r r4`; and validation data: $r_{start} =$ `r r5`, $r_{end} =$ `r r6`)]. 

Candidates who had not been assessed when they responded to the survey were also asked to report their intention to be assessed at that point in time. Analysis of these data (reported in [Appendix D](#exp-int)) suggest that candidates with a higher intention to be assessed were more likely to be assessed six months after answering the survey than those who reported a lower intention to be assessed. These results support the hypothesis, from the theory of planned behaviour, that intentions cause behaviours rather than this feature having been selected due to attribution bias.

Self-efficacy theory [@Bandura1977; @Bandura1982] may also be useful in understanding the relationships between some of the variables selected as important discriminatory variables. Perceived self-efficacy is one's confidence in their abilities to attain a specific outcome [@Bandura1997]. Higher levels of self-efficacy have been associated with higher levels of goal progress [@Sheldon1998], task engagement [@Caraway2003; @Walker2006], goal commitment for self-set goals [@Locke1984; @Locke1990], and on-task effort [@Bandura1983]. The data presented in Table \@ref(tab:male-GTA-18m-combined-summary-stats) show that, on average, male candidates who were assessed within 18 months of their training course felt more confident in their ability to become a Mountain Leader than they did to achieve other personal life goals, whilst those who were not assessed felt less or equally confident, in their ability to become a Mountain Leader in comparison to other personal life goals.

There is also evidence that goal importance influences how committed to a goal an individual will be [@Gollwitzer1993] and that individuals will engage in task-relevant behaviours when they feel that the task is important [e.g., @Ingledew2005; @Yukl1999; @Yukl1996]. Further, there is evidence for an interactive effect between self-efficacy and goal importance where when goal importance is low, self-efficacy had a weaker effect on goal progress, including in situations where multiple goals are considered [e.g., @Beattie2015; @Kernan1990; @Orbell2001; @Schmidt2007; @Schmidt2009; @Schmidt2009a]. This would suggest that more important a candidate feels it is that they become a Mountain Leader, the more committed they will be to achieving it, the more they will engage in task-relevant behaviours (e.g., preparing for an assessment), and that goal progress would be greatest amongst candidates who felt it was both important that they became a Mountain Leader and also felt that they were able to. 

Preparation for an assessment will likely include deliberate practice of the skills required for candidates to look after themselves and others in steep ground and when crossing rivers. For some candidates, these skills will be the most specialist mountaineering skills they possess, and they will have little reason, beyond passing a Mountain Leader assessment, to practice them. Unless these candidates have spent time deliberately preparing for an assessment, it is likely that they will feel less confident than their ideal-self would at assessment that they can successfully demonstrate these skills. Therefore, these candidates may feel that they are not ready to pass an assessment and therefore not attend one, a phenomena described by the participants in the [qualitative chapter](#ml-qualitative).

Self-efficacy theory would suggest that previous performance accomplishments, followed by vicarious experiences, verbal persuasion, and emotional arousal have the greatest effect on self-efficacy and a negative experience of a given magnitude will have a greater effect than an equivalent positive experience [@Bandura1977; @Bandura1982]. Candidates who have visited different mountainous regions are more likely to have encountered a range of different terrain and situations that require them to practice their skills---not limited to the specialist mountaineering skills previously mentioned. In doing so, they may have had mastery experiences in a range of settings that increased their levels of self-efficacy to become a Mountain Leader and to carry out tasks related to the assessment.

There is a broad literature reporting the benefits of resilience [e.g., @Seery2016; @Smith2008]. Becoming a Mountain Leader is a difficult process which requires the investment of time, energy, and money and most candidates will have to deal with setbacks during this process. Candidates who are more resilient will be better able to overcome adversity [@Smith2008]. For example, bad weather on a training course or changes in life circumstances that become barriers to becoming a Mountain Leader. It is also a central tenet of self-efficacy theory that people with firmly established self-efficacy beliefs are more resilient [@Bandura1997] as the stronger self-efficacy beliefs are, the easier they are to maintain following disconfirming events. 

In our analyses we used the time of year that courses took place as a proxy measurement of weather and daylight hours. We would expect that courses nearer to the New Year would have worse weather and less daylight than those nearer to the middle of the year (i.e., June/July). Given that candidates who were trained closer to the middle of the year were more likely to have been assessed 18 months post-training, we would suggest that better weather and more daylight on the training course provides candidates with a more positive experience and possibly a better learning environment. To investigate this further, weather data (held on CMS) and daylight hours data should be included in any future studies.

Social support is typically seen as beneficial to performance and self-confidence [@Rees2007]. It may therefore be surprising that candidates' who were not assessed felt that they had more esteem support available to them. One explanation is that esteem support may be reminding male candidates who are unprepared that they are not ready for an assessment. Similar findings, involving the somewhat paradoxical effects of psychological skills and strategies on performance [@Roberts2013], have been reported elsewhere in the literature and do highlight that some support strategies might need to be utilised with caution.


### Female Candidates - Getting to Assessment.

There were two predictive models retained that could correctly classify female candidates as having been assessed or not within 18 months of their training course with very good accuracy (up to 94.64%). There were `r length(g5_female_gta_18m_retained_vars_unique)` unique variables between the models, `r numbers2words(c(g5_male_gta_18m_retained_vars_unique, g5_female_gta_18m_retained_vars_unique) %>% table() %>% as_tibble() %>% filter(n == 2) %>% nrow())` of which were also selected in the final models for classifying male candidates.

The relationships discussed above for male candidates between perceived available time to become a Mountain Leader, perceived progress towards becoming a Mountain Leader (both absolute and relative to other life goals), importance of becoming a Mountain Leader (both absolute and relative to other life goals), and progress towards becoming a Mountain leader relative to other life goals and getting to assessment within 18 months of training are likely to be the same for female candidates. However, there are other variables, from various stages of the pathway, that are also important for discriminating female candidates who are assessed 18 months post-training from those who are not.

In this study, we asked candidates to state two reasons that they had registered for the Mountain Leader qualification (i.e., two participatory motives). For their first reason, there was little variation and most candidates said that they had registered in order to become a Mountain Leader (n.b., this is an extrinsic participatory motive). Female candidates who gave an extrinsic participatory motive for their second motive (e.g., "To gain employment.”) rather than a more intrinsic one (e.g., "To spend more time in the mountains.") were more likely to have been assessed 18 months after their training course. This finding suggests that having more than one extrinsic participatory motive is important for candidates getting to assessment.

@Mallett2004 presented an explanation for such findings that sits within a the self-determination theory perspective of motivation. They suggested that for some elite athletes, financial reward was perceived as an indicator of competence rather than being perceived as controlling their behaviour. According to cognitive evaluation theory [@Deci1985b] framing financial reward in this way would foster intrinsic motivation. If this hypothesis were to be supported in the present study, and female candidates felt that becoming a Mountain Leader was an indicator of their competence, we would expect female candidates who had been assessed 18 months post-training to rate their participatory motives as more intrinsically than externally regulated, however, this was not the case^[Assessed, (scale 0-100): $M_{intrinsic} =$ `r printnum(mean(filter(G5_female_GTA_18m_train, Assessed_18months == 1)$IntrinsicMotive))`, $SD_{intrinsic} =$ `r printnum(sd(filter(G5_female_GTA_18m_train, Assessed_18months == 1)$IntrinsicMotive))`; $M_{external} =$ `r printnum(mean(filter(G5_female_GTA_18m_train, Assessed_18months == 1)$ExternalMotive))`, $SD_{external} =$ `r printnum(sd(filter(G5_female_GTA_18m_train, Assessed_18months == 1)$ExternalMotive))`.]. 

```{r}
G5_female_GTA_18m_train %>% 
  group_by(Assessed_18months) %>% 
  summarise(intrinsic = mean(IntrinsicMotive),
            introjected = mean(IntrojectedMotive),
            external = mean(ExternalMotive))
```


Another explanation for this finding is that the study did not include variables that were relevant, and these results are spurious. There are two main arguments that would support this view. Firstly, candidates were asked to list their reasons for registering and that does not mean that they did not later develop more autonomous forms of motivation. For example, perceived competence, motivational feedback, and perceived control have all been related to intrinsic motivation [@Guay2001; @Vallerand2001] and course staff should be able to positively influence each of these by coaching their candidates, thus fostering the development of more autonomous motivation. However, for this hypothesis to be supported, candidates who were more intrinsically motivated at the registration would likely need to become more extrinsically motivated in the same period. It would seem unlikely that the behaviour of course staff would cause such a reversal of candidates' motivation.

A third explanation is that whilst the motivation to become a Mountain Leader was extrinsic, this was a sub-goal of an intrinsic goal. However, postulate four of Vallerand's [-@Vallerand1997] hierarchical model of motivation suggests that "there is a recursive bottom up relationship between motivation at one level and motivation at the next higher level in the hierarchy"---the opposite direction to that which would be suggested by this explanation. 

This finding does not fit comfortably with the traditional view that predominantly intrinsic motives are better predictors of performance and goal persistence than predominantly extrinsic motives. However, there is evidence from the sporting domain that self-determined forms of motivation are not always the best predictors of a desired outcome [e.g., @Chantal1996; @Fortier1995; @Gullich2019; @Hardy2017].

Goal setting has been shown to improve outcomes in several domains [see @Weinberg2014 p 356]. One way that goal setting facilitated by training course staff may have helped candidates is by enabling them to maximise the benefits of the time that they spent consolidating their skills and preparing for a Mountain Leader assessment after the training course. In addition to this, goal setting  may have made it more likely that candidates would prepare for an assessment. The more specific these goals are, the more they will have focused candidates’ attention and efforts towards being at the right level to pass an assessment. Further, goal setting will have helped facilitate mastery experiences thus, this goal setting will have helped female candidates develop their confidence, which as discussed below, is key for female candidates getting to assessment.

Female candidates who experienced professional change post-training were less likely to be assessed 18 months post-training that those who did not. For male candidates, we suggested that experiencing social change post-training may reduce the time that they have available to become a Mountain Leader; this may also be true for female candidates who experience professional change. An additional consideration for those who experience professional change, is that if they were pursuing the Mountain Leader qualification for work, they may no longer need to become a Mountain Leader. When considered with the apparent importance of having extrinsic participatory motivation---often for work---this explanation seems plausible and would be characterised by reduced levels of extrinsic motivation and increased levels of amotivation.

Female candidates who were assessed 18 months post-training felt that they had done more to prepare for an assessment in the six-months prior to their assessment than those who had not been assessed felt that they had done in the six-months prior to completing the survey. In addition, they were more likely to have included specific navigation practice in their preparation.

Variables related to three different sets of skills were identified as important for discriminating those who did and did not pass their first assessment: navigation, security in steep ground, and emergency procedures. It is important for candidates to be competent in all three of these areas. In general, the more experience one has the more confident and competent they will be. Whilst the relationship between confidence and experience may vary between individuals [@Weinberg2014], mastery experiences will enhance one's confidence [@Bandura1977].

```{r include=FALSE}
G5_female_GTA_18m_train %>% 
  group_by(Assessed_18months, as.factor((IdealNumQMDs > 40 & OughtNumQMDs > 40))) %>% 
  count()

G5_female_GTA_18m_train %>% 
  transmute(self_guide_high = as.factor((IdealNumQMDs > 40 & OughtNumQMDs > 40)),
            Assessed_18months = Assessed_18months) -> self_guide_df

# G5_female_GTA_18m_train %>% 
#   select(IdealNumQMDs, OughtNumQMDs, PPMountainLeaderImportance, PPMountainLeaderEfficacy, PPMountainLeaderProgress, Assessed_18months) %>% 
#   write_csv("G5_female_GTA_18m_train_QMD_self_guides.csv")
```

In the model developed from the survey data, the number of QMDs that a candidate felt their self-guides would have logged before assessment were included as important discriminatory variables. The greater the number of QMDs logged by a self-guide at assessment, the greater the discrepancy post-training. According to self-discrepancy theory, those with greater discrepancies would be more motivated than those with smaller discrepancies [@Higgins1987] and they may therefore be more likely to prepare for an assessment. It is also possible that the higher goals (as represented by the self-guides) of the candidates who were assessed within 18 months of their training course were a result of them having higher-levels of self-efficacy [cf. @Bandura1986]. 

Due to the cross-sectional design of this study, it is not possible to empirically test the direction of causality between motives, confidence, and experience. However, we asked candidates to think about themselves at different points in time (i.e., at registration, training, and pre-assessment) whilst completing the survey and to answer the questions in relation to how they remember feeling at those time points. Assuming that they did this and that attribution bias did not influence their answers, we would suggest that their goals (as represented by their self-guides) influenced their motivation to gain experience, as candidates who set themselves higher goals would have greater levels of motivation to reduce the discrepancy between their actual-self and their self-guides. If this greater motivation resulted in candidates gaining more experience as they prepared for an assessment, they would likely have more mastery experiences, resulting in higher levels of self-efficacy.

It is interesting to note that further inspection of the data revealed that only candidates who were not assessed within 18 months of their training course felt that their ideal- or ought-self would have logged fewer than 40 QMDs pre-assessment---the prerequisite number set by Mountain Training---and that candidates who felt both of their self-guides would have logged more than 40 QMDs pre-assessment were more likely to have been assessed 18 months post training than those who did not feel that both of their self-guides would have more than 40 QMDs at assessment^[These thresholds were identified by applying a J48 classifier to the ideal- and ought-self number of QMDs pre-assessment, which had a classification rate of 72.22% and a balanced confusion matrix. When assessed using a $\chi^2$-test, those who exceed these thresholds are significantly more likely to have been assessed 18 months post-training that those who do not, `r apa_print.htest(chisq.test(x = self_guide_df$self_guide_high, y = self_guide_df$Assessed_18months), n = nrow(self_guide_df))$statistic`.].

In the model developed from the full set of features (i.e., including survey and DLOG variables), neither of the self-guide features, nor the discrepancy scores between the self-guides and the number of QMDs logged 18 months post-training were selected as important discriminatory variables. However, several DLOG variables were, which suggests that experience is more important than motives. The DLOG variables selected were all related to QMDs and no other types of experience. Candidates who had been assessed 18 months post-training had more logged experience (as represented by the number of QMDs and QMD entries), had gained that experience in a greater range of locations (number of areas and number of mountain regions), had more logged more experience in poor visibility, and had summited a greater number of mountains (in total and mountains higher than 600m above sea level). This set of features suggest that it is not only important that female candidates gain experience during their preparation for assessment, but this experience should be varied and include experience where their skills will be challenged, for example having to navigate due to poor visibility.

Differences in the features included in the two models may be informative. In the model developed from survey data only, the number of QMDs logged by both self-guides before assessment were included as discriminatory variables, but they were not included in the model developed with the survey and DLOG data although various DLOG variables were. This difference in model structure suggests that actual experience is more important for discriminating female candidates who have been assessed 18 months post-training from those who have not. Similarly, both models included features that relate to navigation, the model developed from survey data included candidates having included navigation in their preparation for an assessment and their self-efficacy to navigate in any conditions, however, when DLOG (logged experience) variables were included the number of QMDs logged in poor visibility---which would necessitate candidates using their navigation skills---were included but the two in the former model were not. This suggests that navigation skills are important and are likely developed through practicing them in realistic conditions, and that the continuous behavioural measure of experience is a better discriminator than the combination of the binary self-report measure of deliberate practice^[The continuous behavioural measure (DLOG variables) may capture this information in a greater level of detail than the binary self-report variable and therefore be chosen as a better predictor in the pattern recognition analyses.] and self-report pre-assessment self-efficacy.

The exact nature of the relationship between the features relating to candidates' navigation skills/experience and the outcome is unclear. For example, a candidate may decide that they want to be assessed and must therefore practice their navigation skills, possibly by going into the mountains in poor visibility conditions in order to prepare for an assessment, or they may practice their navigation skills and having gained experience, decide that they would like to attend an assessment. However, when considering the results from the qualitative chapter, it is likely that this deliberate practice increases female candidates' efficacy to become MLs.

```{r eval=FALSE}

G5_female_GTA_18m_train %>% 
  #filter(OughtNumQMDs < 75) %>% 
  ggplot(aes(x = IdealNumQMDs, y = OughtNumQMDs, 
             colour = as.factor(Assessed_18months), 
             shape = as.factor(CandidateId %in% G5_female_GTA_18m_train$CandidateId),
             group = as.factor(Assessed_18months))) +
  geom_jitter(width = .4, height = .4) +
  geom_smooth(method = "lm") +
  coord_cartesian(xlim = c(20,100), ylim = c(20,100)) +
  guides(shape = FALSE, colour = FALSE) -> p 

ggMarginal(p = p, type = "histogram", groupColour = TRUE)


G5_female_GTA_18m_train %>% 
  ggplot(aes(x = as.factor(Assessed_18months), y = t18_ought_QMD_diff)) +
  geom_violin() +
  geom_jitter(aes(shape = as.factor(OughtNumQMDs > 40)))

G5_female_GTA_18m_train %>% 
  select(Assessed_18months, OughtNumQMDs) %>% 
  arrange(desc(OughtNumQMDs))

```


### Passing First Time

We retained two predictive models that could classify candidates as having passed their first assessment or not with good accuracy (up to 82.61%). None of the features selected as important discriminatory variables. There were `r length(g5_ftp_rfe_classification_vars_unique)` unique variables between the models, none of which were included in ether the female or male getting to assessment models. The features included in the final models can be considered under two broad headings, candidates experience of training and their preparation for assessment.

Candidates who felt that course staff were more involved with their development, observed them more closely, and help them to set goals and identify targets for attaining their goals were more likely to pass their first assessment than those who felt the course staff engaged in these behaviours to a lesser degree. Coaching usually aims to improve an individual's knowledge, skills, and competencies [@Wagstaff2018]. Course staff will engage in coaching behaviours to a greater or lesser extent and their facilitation of candidate's development will vary accordingly. Two coaching behaviours were selected as important discriminatory variables: effective questioning and goal setting. It is important to note that candidates who did not pass their first assessment reported that their course staff did engage in these behaviours, just not to the same extent as those who did pass.

Candidates feeling that their course staff were genuinely interested in their development would rate them higher on the involvement dimension of basic psychological needs supportive behaviours [@Markland2010]. Clear and specific goals are more effective than broad/vague goals for influencing behaviour change [@Gould2005] and by closely observing a candidate's skills, course staff will have more information with which to help the candidate set goals. The use of effective questioning may encourage candidates to think and reflect on their actions, thus encouraging self-directed learning, thus supporting candidates' autonomy [@Wagstaff2018].

We measured relationship and process conflict between candidates, and between candidates and staff on the training course and it was the total of these four items that discriminated between candidates who did and did not pass, suggesting that any conflict experienced on a training course has a negative impact on the assessment outcome. Relationship and process conflict have generally been found to have negative relationships between both distal- and proximal-group outcomes [see @deWit2012]. Whilst candidates on a Mountain Leader training course may only be a group for the five days of the course, they will share goals and be required to work together to complete tasks. It has been suggested that process and relationship conflict can distract groups from task accomplishment [@Evan1965; @Jehn1995]. In the context of the Mountain Leader training course, this conflict may manifest as less time being spent on course content and the training staff coaching the candidates.

Candidates who felt they had more informational support available to them were more likely to have felt that they would have someone to give them advice about becoming a Mountain Leader and about performing at assessment if they needed it. In the sporting domain, perceived available support has been associated with positive effects on self-confidence and stress buffering [@Rees2007], performance [@Freeman2009], and the processes underpinning performance [@Rees1999]. More specifically, perceived informational support has been correlated with greater situational control, increased challenge appraisal, decreased threat appraisal, and better performance [@Freeman2009].

The results of the qualitative study suggested that the quantity, quality, and variety of experience candidates had when they were assessed were all important for discriminating those who did pass from those who did not. Interestingly, no QMD variables were selected in the final models, however, Hill/Moorland Walking, Mountain Walking, and Alpine Climbing variables were selected. Hill/Moorland Walking and Mountain Walking are types of mountaineering experience that do not meet the standard for a QMD because of the terrain that they cover, or there has not been sufficient challenge to develop candidates Mountain Leader skills and therefore do not qualify as a QMDs. In contrast, Alpine Climbing is considered beyond the scope of the Mountain Leader qualification but will include terrain that will develop skills relevant to the Mountain Leader assessment (e.g., moving safely in steep ground).

The results presented above suggest that candidates who include Hill/Moorland Walking and Mountain Walking in their DLOG are less likely to pass. One explanation for this finding is that candidates who feel they have a weak logbook want to show all the experience that they feel is relevant, whereas a candidate who feels they have a strong logbook may only feel the need to include the experience they feel is most relevant (e.g., QMDs). Further, some candidates will include every QMD that they have, whereas some may only include the minimum needed for the course despite having many more. This may explain why QMD variables are not included and Hill/Moorland Walking and Mountain Walking variables are, but with a counter-intuitive effect.

Seven of the 23 candidates who did not pass their first assessment were only deferred because they had too few QMDs in their logbook at assessment^[Candidates who are deferred must be reassessed, however, when they are deferred for not having enough experience at assessment, their reassessment is simply presenting an assessor with an updated logbook.]. It is important to highlight that the features presented here discriminate between candidates who do and do not pass their assessment, not between candidates who are and are not good enough to pass a Mountain Leader assessment, in terms of their skills and decision making. If we removed these particular candidates from the sample, we would have too few cases to perform the analysis, therefore, it is difficult at this juncture to answer the question "Is having more than the minimum experience beneficial for passing a Mountain Leader assessment." If anything, it is evidence that one can pass the practical element a Mountain Leader assessment with fewer than 40 QMDs and may be a contributory factor to the lower classification rates in the passing first time analyses than the getting to assessment analyses.

Candidates who live further from the mountains were less likely to pass; of the 12 candidates who lived more than 16 hours from the nearest mountains, only one passed their first assessment (`r printnum(100*1/12)`%), whereas of the 34 candidates who lived within six hours of the nearest mountains, 22 passed their first assessment (`r printnum(100*22/34)`%). Living further from the mountains may mean candidates are not able to travel to the mountains as frequently and may not be able to spend as long there when they do travel. This potential barrier may result in candidates logging more experience that is not as relevant (e.g., Hill/Moorland Walking) or not of the requisite quality (e.g., Mountain Walking) during their preparation for an assessment.


### Summary

It is important for both all candidates that they feel that they have the time to become a Mountain Leader and that becoming a Mountain Leader is an important life goal relative to other life goals that they may be pursuing concurrently. It is also vital that candidates prepare effectively for an assessment. The more variety there is in this preparation the better, as such preparation will likely include practicing skills, which candidates may only require to pass the Mountain Leader assessment, thus increasing their confidence that they can perform tasks associated with those skills at assessment. Experiencing changes in life may "derail" candidates' progress towards becoming a Mountain Leader^[The results of the qualitative chapter would have suggested that female candidates may not be assessed if they have children---listed as a social change in the survey---however, social changes were more important for male candidates. Other studies have found that the effect of having children was greater for males than females and suggested that this may be because the changes to men's lives are greater than they are for women. [e.g., @VandeSchoot2013a; @Waite1995].], especially if this precipitates a change in the relative importance of becoming a Mountain Leader. 

Both experience and self-efficacy appear to be more important variables for discriminating female candidates who are assessed within 18 months of their training course than  for male candidates. Future studies should consider the nature of the relationship between experience and self-efficacy and potential gender-differences.


### Limitations

Several limitations can be identified in this project. Firstly, most of the data used was collected retrospectively. Retrospective data will be less accurate as time increases between the event and when participants are sampled, and people may create their own narrative retrospectively which may or may not reflect reality. An example of this could be a candidate who did not pass their first assessment attributing their failure to the coaching (or lack thereof) they received on their training course.

Secondly, there is some evidence of sampling bias in the data used to identify the important discriminatory factors for both getting to assessment and passing. The proportion of female and male candidates who did get to assessment within 18 months of their training course is not the same in the retrospective data (females = 23.21% and males = 41.35%) as it is in the population of candidates trained in the same period (females = 19.02% and males = 30.22%). In addition to this, the proportion of males who did not pass their first assessment is not the same in the retrospective data (13.5%) as it is in the prospective data (19.6%) or in the population^[Candidates who were first trained after 2016.] (19.8%); there is no evidence of the same problem in the data collected from female candidates. The simplest explanation for this is that candidates who are not assessed and male candidates who do not pass their first assessment are less likely to *retrospectively* respond to the survey. 

Whilst there may be a subset of candidates who are not represented in the data collected as part of this project, we believe that the findings presented in this report can be used to make a positive impact on the completion rate of the Mountain Leader qualification. This belief is based not only on the analyses of retrospective and prospective data presented here, but their congruence with the results from the initial qualitative study and existing literature.

Further analysis of these data in the future should mitigate this sampling bias so that the response rate in the prospective data is similar to that in the population and reduce the impact of recall bias. However, a truly prospective study that collected data from candidates at registration, training, and during their consolidation would likely overcome the limitations described above.


## Potential Implications

If candidates leave their training course wanting to be assessed (i.e., they are motivated) and they understand what they need to do in order to get to an assessment (i.e., they have a plan) they are more likely to be assessed. Any plan for getting to assessment that a candidate creates should consider how becoming a Mountain Leader fits into the rest of their life as this is clearly very important. Such a process needs to include consideration of other life goals, how much time candidates feel they have available to prepare for an assessment, and how easy it is for them to access the mountains. It is important that these considerations are made, as if a plan is unrealistic it is unlikely to be followed and failure to attain goals will have a negative impact on candidates' motivation and confidence. Similarly, it is important that candidates who experience changes in their life reconsider their plan, as the changes they have experienced may then make the plan unrealistic.

Candidates' plans for consolidation should include goals that provide them with the opportunity for mastery experiences. When set in conjunction with course staff who can provide structure. These goals will offer candidates the opportunity to experience success as a result of good performance, which should increase their level of confidence to perform tasks related to passing a Mountain Leader assessment. Course staff may benefit from additional training, aimed at enhancing their coaching skills and helping them to provide a need supportive environment for their candidates.


## Conclusion

From up to 529 variables, we were able to identify predictive models of no more than 16 variables, that could correctly classify candidates with up to 94.64% accuracy, based on three different outcome variables with good to excellent accuracy. The discriminatory variables included in these models covered several different temporal aspects of the training pathway and related to both candidates and course staff. This study supports the view of previous research that the development of expertise is multi-faceted and complex.

In order to become a Mountain Leader, these findings suggest that it is important that candidates feel that becoming a Mountain Leader is an important goal, are able to prepare effectively for an assessment including gaining relevant experience, feel confident that they can perform Mountain Leader related tasks at an assessment, and have a positive experience of the training course. These findings leave Mountain Training with several areas to focus their efforts if they wish to improve the completion rates of the Mountain Leader qualification.
